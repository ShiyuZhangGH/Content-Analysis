{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings Supplemental\n",
    "\n",
    "This notebook contains two additional uses for word embeddings\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "import copy\n",
    "\n",
    "#gensim uses a couple of deprecated features\n",
    "#we can't do anything about them so lets ignore them \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import docx \n",
    "\n",
    "import re \n",
    "import urllib.parse \n",
    "import io \n",
    "import json\n",
    "import os.path\n",
    "import os \n",
    "import time\n",
    "import nltk \n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "import _pickle as cPickle\n",
    "\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy \n",
    "import scipy.cluster.hierarchy\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import matplotlib.cm #Still for graphics\n",
    "import seaborn as sns #Makes the graphics look nicer\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning, it\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. \n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = resume_model.index2word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hiringOrganization_organizationName</th>\n",
       "      <th>jobDescription</th>\n",
       "      <th>jobLocation_address_region</th>\n",
       "      <th>jobLocation_geo_latitude</th>\n",
       "      <th>jobLocation_geo_longitude</th>\n",
       "      <th>qualifications</th>\n",
       "      <th>responsibilities</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158844</td>\n",
       "      <td>Golfsmith International, Inc.</td>\n",
       "      <td>\"Sales Associate Tracking Code 220425-971 Job ...</td>\n",
       "      <td>California</td>\n",
       "      <td>33.91918</td>\n",
       "      <td>-118.41647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ensure each Customer receives exceptional ser...</td>\n",
       "      <td>[[``, Sales, Associate, Tracking, Code, 220425...</td>\n",
       "      <td>[[sales, associate, tracking, code, job, descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257645</td>\n",
       "      <td>Intel</td>\n",
       "      <td>For PHY system engineering team within the Wir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[For, PHY, system, engineering, team, within,...</td>\n",
       "      <td>[[for, phy, system, engineering, team, within,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107875</td>\n",
       "      <td>Florida Hospital</td>\n",
       "      <td>*RN Medical Oncology PCU Orlando - Nights* Flo...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>28.53834</td>\n",
       "      <td>-81.37924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[*RN, Medical, Oncology, PCU, Orlando, -, Nig...</td>\n",
       "      <td>[[medical, oncology, pcu, orlando, florida, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202394</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Specialist Sales Account Representative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Specialist, Sales, Account, Repres...</td>\n",
       "      <td>[[title, specialist, sales, account, represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109675</td>\n",
       "      <td>Footprint Retail Services</td>\n",
       "      <td>**Footprint Retail Services** **Job Descriptio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Merchandiser must complete all assigned merc...</td>\n",
       "      <td>[[**Footprint, Retail, Services**, **Job, Desc...</td>\n",
       "      <td>[[retail, job, title, retail, merchandiser, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>215973</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Position Purpose: Provide outstanding service ...</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>41.13060</td>\n",
       "      <td>-85.12886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provide outstanding service to ensure efficien...</td>\n",
       "      <td>[[Position, Purpose, :, Provide, outstanding, ...</td>\n",
       "      <td>[[position, purpose, provide, outstanding, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>207524</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>The Asset Protection Specialist is primarily r...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>40.21455</td>\n",
       "      <td>-74.61932</td>\n",
       "      <td>Must be eighteen years of age or older. Must p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[The, Asset, Protection, Specialist, is, prim...</td>\n",
       "      <td>[[the, asset, protection, specialist, is, prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64426</td>\n",
       "      <td>East West Bank</td>\n",
       "      <td># Job Description East West Bank is one of the...</td>\n",
       "      <td>California</td>\n",
       "      <td>34.06862</td>\n",
       "      <td>-118.02757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We are currently seeking a Customer Service Ce...</td>\n",
       "      <td>[[#, Job, Description, East, West, Bank, is, o...</td>\n",
       "      <td>[[job, description, east, west, bank, is, one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>245192</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Job Description IBM is seeking to hire a Senio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Job, Description, IBM, is, seeking, to, hire...</td>\n",
       "      <td>[[job, description, ibm, is, seeking, to, hire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202429</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Field Solutions Engineer Location: New ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Job Functions;Specific duties in this role wil...</td>\n",
       "      <td>[[Title, :, Field, Solutions, Engineer, Locati...</td>\n",
       "      <td>[[title, field, solutions, engineer, location,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>269503</td>\n",
       "      <td>J&amp;J Family of Companies</td>\n",
       "      <td>Project Manager (m/w) - Government &amp; Public Af...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Project, Manager, (, m/w, ), -, Government, ...</td>\n",
       "      <td>[[project, manager, government, public, jansse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>139164</td>\n",
       "      <td>Genesis Healthcare</td>\n",
       "      <td>Certified Medicine Aide Area of Interest: Nurs...</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>39.10972</td>\n",
       "      <td>-95.08775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Receives report at the beginning of shift with...</td>\n",
       "      <td>[[Certified, Medicine, Aide, Area, of, Interes...</td>\n",
       "      <td>[[certified, medicine, aide, area, of, interes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>255915</td>\n",
       "      <td>Ingersoll Rand</td>\n",
       "      <td>**Description:** At Ingersoll Rand we're passi...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>35.14953</td>\n",
       "      <td>-90.04898</td>\n",
       "      <td>Considerable knowledge of industry related sup...</td>\n",
       "      <td>Ingersoll Rand is a diverse and inclusive envi...</td>\n",
       "      <td>[[**Description, :, **, At, Ingersoll, Rand, w...</td>\n",
       "      <td>[[at, ingersoll, rand, we, passionate, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>173294</td>\n",
       "      <td>HamiltonConstructionCompany</td>\n",
       "      <td>\"Hamilton Construction,in businesssince 1939 a...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>44.04624</td>\n",
       "      <td>-123.02203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[``, Hamilton, Construction, ,, in, businesss...</td>\n",
       "      <td>[[hamilton, construction, in, businesssince, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116855</td>\n",
       "      <td>G6 Hospitality</td>\n",
       "      <td>Business Unit: DirectEmployers Title: Manager ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>32.83707</td>\n",
       "      <td>-97.08195</td>\n",
       "      <td>High School diploma or equivalent;Computer pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Business, Unit, :, DirectEmployers, Title, :...</td>\n",
       "      <td>[[business, unit, directemployers, title, mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40701</td>\n",
       "      <td>Dollar Tree</td>\n",
       "      <td>Auto req ID 41872BR Title ASSISTANT MANAGER Em...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>32.85791</td>\n",
       "      <td>-97.25474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Auto, req, ID, 41872BR, Title, ASSISTANT, MA...</td>\n",
       "      <td>[[auto, req, id, title, assistant, manager, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>292406</td>\n",
       "      <td>Johns Hopkins Medicine</td>\n",
       "      <td>Johns Hopkins employs more than 20,000 people ...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>39.29038</td>\n",
       "      <td>-76.61219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Johns, Hopkins, employs, more, than, 20,000,...</td>\n",
       "      <td>[[johns, hopkins, employs, more, than, people,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>358904</td>\n",
       "      <td>LHC Group</td>\n",
       "      <td>Job Description As a CNA, you will perform a v...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>34.74481</td>\n",
       "      <td>-87.66753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Your specific duties for this role will includ...</td>\n",
       "      <td>[[Job, Description, As, a, CNA, ,, you, will, ...</td>\n",
       "      <td>[[job, description, as, a, cna, you, will, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>126507</td>\n",
       "      <td>GE</td>\n",
       "      <td>2306307 **Business** GE Capital **Business Seg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Doswiadczenie w pracy na podobnym stanowisku;W...</td>\n",
       "      <td>Prowadzenie projektow zw\\. z rozwojem pracowni...</td>\n",
       "      <td>[[2306307, **Business**, GE, Capital, **Busine...</td>\n",
       "      <td>[[ge, capital, capital, international, bank, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>90538</td>\n",
       "      <td>EY</td>\n",
       "      <td>Title: Greater China TAS Referrals (China Sout...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Greater, China, TAS, Referrals, (,...</td>\n",
       "      <td>[[title, greater, china, tas, referrals, china...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59256</td>\n",
       "      <td>Dr Pepper Snapple Group</td>\n",
       "      <td>Merchandiser The Merchandiser is responsible f...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Merchandiser, The, Merchandiser, is, respons...</td>\n",
       "      <td>[[merchandiser, the, merchandiser, is, respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>85716</td>\n",
       "      <td>Eurofins Lancaster Laboratories</td>\n",
       "      <td>### Eurofins is the world leader in the food, ...</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>39.55895</td>\n",
       "      <td>-84.30411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[#, #, #, Eurofins, is, the, world, leader, i...</td>\n",
       "      <td>[[eurofins, is, the, world, leader, in, the, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>203342</td>\n",
       "      <td>HMSHOST</td>\n",
       "      <td>\"**Having the right ingredients makes all the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Greets customers and takes food order provide...</td>\n",
       "      <td>[[``, **Having, the, right, ingredients, makes...</td>\n",
       "      <td>[[the, right, ingredients, makes, all, the, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>283327</td>\n",
       "      <td>Jewel-Osco</td>\n",
       "      <td>JOB: Retail Clerk  Liquor/Beer/Wine JOB OVERVI...</td>\n",
       "      <td>Montana</td>\n",
       "      <td>48.19579</td>\n",
       "      <td>-114.31291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sells, receives, stocks and may order beer/liq...</td>\n",
       "      <td>[[JOB, :, Retail, Clerk, Liquor/Beer/Wine, JOB...</td>\n",
       "      <td>[[job, retail, clerk, job, overview, provides,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>320876</td>\n",
       "      <td>KeyPoint Government Solutions</td>\n",
       "      <td>**Overview:** KeyPoint Government Solutions is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[**Overview, :, **, KeyPoint, Government, Sol...</td>\n",
       "      <td>[[keypoint, government, solutions, is, current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>311069</td>\n",
       "      <td>Kelly Services</td>\n",
       "      <td>**Strategic Sales Lead  Premier Brands and Con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bachelors degree (MBA preferred) or equivalent...</td>\n",
       "      <td>This is a field based corporate sales position...</td>\n",
       "      <td>[[**Strategic, Sales, Lead, Premier, Brands, a...</td>\n",
       "      <td>[[sales, lead, premier, brands, and, consumer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>141835</td>\n",
       "      <td>Genesis Rehabilitation</td>\n",
       "      <td>Job Title: Occupational Therapist Area of Inte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Screens, examines and evaluates patients, incl...</td>\n",
       "      <td>[[Job, Title, :, Occupational, Therapist, Area...</td>\n",
       "      <td>[[job, title, occupational, therapist, area, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>202406</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Pentaho-Sales Executive-Texas Location:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Pentaho-Sales, Executive-Texas, Lo...</td>\n",
       "      <td>[[title, location, texas, the, regional, accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>111597</td>\n",
       "      <td>Franciscan St. Eilzabeth Health</td>\n",
       "      <td>Title: Respiratory Therapy, Intern Location: X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Respiratory, Therapy, ,, Intern, L...</td>\n",
       "      <td>[[title, respiratory, therapy, intern, location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>260448</td>\n",
       "      <td>Intel</td>\n",
       "      <td>The Influencer Sales Group Solution Architect ...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>45.52289</td>\n",
       "      <td>-122.98983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[The, Influencer, Sales, Group, Solution, Arc...</td>\n",
       "      <td>[[the, influencer, sales, group, solution, arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>197429</td>\n",
       "      <td>Hill-Rom</td>\n",
       "      <td>Title: Contract Svc Admin Location: United Sta...</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>42.78920</td>\n",
       "      <td>-85.51669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>_ Other duties may be assigned:_</td>\n",
       "      <td>[[Title, :, Contract, Svc, Admin, Location, :,...</td>\n",
       "      <td>[[title, contract, svc, admin, location, unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>359233</td>\n",
       "      <td>LHC Group</td>\n",
       "      <td>Job Description The Occupational Therapist is ...</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>41.51337</td>\n",
       "      <td>-87.67421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Job, Description, The, Occupational, Therapi...</td>\n",
       "      <td>[[job, description, the, occupational, therapi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5520</td>\n",
       "      <td>DCS Corporation</td>\n",
       "      <td>Systems Engineer - SE2-3678 Location: AL, Hunt...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>34.73037</td>\n",
       "      <td>-86.58610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Participate in preparation of technical and pr...</td>\n",
       "      <td>[[Systems, Engineer, -, SE2-3678, Location, :,...</td>\n",
       "      <td>[[systems, engineer, location, al, huntsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>205825</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Position Purpose:Associates in Office/Store Su...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.12509</td>\n",
       "      <td>-72.74954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usually in a comfortable environment but with ...</td>\n",
       "      <td>[[Position, Purpose, :, Associates, in, Office...</td>\n",
       "      <td>[[position, purpose, associates, in, support, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>68546</td>\n",
       "      <td>Education Corporation of America</td>\n",
       "      <td># Description Education Corporation of America...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>33.52066</td>\n",
       "      <td>-86.80249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education Corporation of America owns and oper...</td>\n",
       "      <td>[[#, Description, Education, Corporation, of, ...</td>\n",
       "      <td>[[description, education, corporation, of, ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>141067</td>\n",
       "      <td>Genesis Rehabilitation</td>\n",
       "      <td>Job Title: Physical Therapist Area of Interest...</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>42.99564</td>\n",
       "      <td>-71.45479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis Rehabilitation Services is looking for...</td>\n",
       "      <td>[[Job, Title, :, Physical, Therapist, Area, of...</td>\n",
       "      <td>[[job, title, physical, therapist, area, of, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>26925</td>\n",
       "      <td>Destination Hotels &amp; Resorts</td>\n",
       "      <td>Busser Property Paradise Point Resort &amp; Spa Co...</td>\n",
       "      <td>California</td>\n",
       "      <td>32.71533</td>\n",
       "      <td>-117.15726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Busser, Property, Paradise, Point, Resort, &amp;...</td>\n",
       "      <td>[[busser, property, paradise, point, resort, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>81637</td>\n",
       "      <td>Epic Health Services</td>\n",
       "      <td>Epic Health Services, Inc. is looking for Bili...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>39.95373</td>\n",
       "      <td>-74.19792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provide skilled nursing care to pediatric and ...</td>\n",
       "      <td>[[Epic, Health, Services, ,, Inc., is, looking...</td>\n",
       "      <td>[[epic, health, services, is, looking, for, bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>191317</td>\n",
       "      <td>Hewlett Packard Enterprise Company</td>\n",
       "      <td>Title: Software Engineer Location: China-Shang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Software, Engineer, Location, :, C...</td>\n",
       "      <td>[[title, software, engineer, location, other, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>134395</td>\n",
       "      <td>Genesis Healthcare</td>\n",
       "      <td>Registered Nurse Area of Interest: Nursing - R...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.57509</td>\n",
       "      <td>-70.93005</td>\n",
       "      <td>Registered Nurse / RN Requirements:;Bachelor's...</td>\n",
       "      <td>Twin Oaks in Danvers MA is looking for a Per D...</td>\n",
       "      <td>[[Registered, Nurse, Area, of, Interest, :, Nu...</td>\n",
       "      <td>[[registered, nurse, area, of, interest, nursi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>177803</td>\n",
       "      <td>Harris Corporation</td>\n",
       "      <td>Description: Job Title: Electrical Engineer &amp;n...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>28.03446</td>\n",
       "      <td>-80.58866</td>\n",
       "      <td>Master&amp;rsquo s degree in electrical engineerin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Description, :, Job, Title, :, Electrical, E...</td>\n",
       "      <td>[[description, job, title, electrical, enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>222835</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>\"Position Purpose: Cashiers play a critical cu...</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>36.72806</td>\n",
       "      <td>-108.21869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usually in a comfortable environment but with ...</td>\n",
       "      <td>[[``, Position, Purpose, :, Cashiers, play, a,...</td>\n",
       "      <td>[[position, purpose, cashiers, play, a, critic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>12790</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Deloitte is one of the leading professional se...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>39.95234</td>\n",
       "      <td>-75.16379</td>\n",
       "      <td>Establishes deliverable structure and content ...</td>\n",
       "      <td>Job Responsibilities;In providing finance-rela...</td>\n",
       "      <td>[[Deloitte, is, one, of, the, leading, profess...</td>\n",
       "      <td>[[deloitte, is, one, of, the, leading, profess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>319987</td>\n",
       "      <td>Kettering Medical Center</td>\n",
       "      <td>Description Greets patients and visitors in a ...</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>39.68950</td>\n",
       "      <td>-84.16883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Description, Greets, patients, and, visitors...</td>\n",
       "      <td>[[description, greets, patients, and, visitors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>352211</td>\n",
       "      <td>Learning Care Group</td>\n",
       "      <td># Job Description Join our talented team, wher...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>33.42227</td>\n",
       "      <td>-111.82264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join our talented team, where we inspire child...</td>\n",
       "      <td>[[#, Job, Description, Join, our, talented, te...</td>\n",
       "      <td>[[job, description, join, our, talented, team,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>233278</td>\n",
       "      <td>Humana</td>\n",
       "      <td>Role: Technology Architect Assignment: IT Loca...</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>38.25424</td>\n",
       "      <td>-85.75941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guide all aspects of design, implementation an...</td>\n",
       "      <td>[[Role, :, Technology, Architect, Assignment, ...</td>\n",
       "      <td>[[role, technology, architect, assignment, it,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>338881</td>\n",
       "      <td>Kronos Incorporated</td>\n",
       "      <td>* Observes and reports activities and incident...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>36.32311</td>\n",
       "      <td>-86.71333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[*, Observes, and, reports, activities, and, ...</td>\n",
       "      <td>[[observes, and, reports, activities, and, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>53773</td>\n",
       "      <td>Dominos Pizza</td>\n",
       "      <td>\"ABOUT THE JOB Do you know why Domino's Pizza ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>General job duties for all store team members;...</td>\n",
       "      <td>\"You must be 18 years of age and have a valid ...</td>\n",
       "      <td>[[``, ABOUT, THE, JOB, Do, you, know, why, Dom...</td>\n",
       "      <td>[[about, the, job, do, you, know, why, domino,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>158203</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>Registered Dietician at AseraCare Hospice work...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Registered, Dietician, at, AseraCare, Hospic...</td>\n",
       "      <td>[[registered, dietician, at, aseracare, hospic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>156994</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>At Golden LivingCenters, we care for every pat...</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>44.08054</td>\n",
       "      <td>-103.23101</td>\n",
       "      <td>Currently licensed or registered in state of p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[At, Golden, LivingCenters, ,, we, care, for,...</td>\n",
       "      <td>[[at, golden, livingcenters, we, care, for, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>249542</td>\n",
       "      <td>ICF International</td>\n",
       "      <td>Energy Efficiency and Low Income Representativ...</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>42.27756</td>\n",
       "      <td>-83.74088</td>\n",
       "      <td>/ /;/ /</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Energy, Efficiency, and, Low, Income, Repres...</td>\n",
       "      <td>[[energy, efficiency, and, low, income, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>173219</td>\n",
       "      <td>Hallmark Health</td>\n",
       "      <td>Office Coordinator Geriatric Assessment Center...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.41843</td>\n",
       "      <td>-71.10616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School Diploma or equivalent. Bachelor's ...</td>\n",
       "      <td>[[Office, Coordinator, Geriatric, Assessment, ...</td>\n",
       "      <td>[[office, coordinator, geriatric, assessment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>50969</td>\n",
       "      <td>Dominos Pizza</td>\n",
       "      <td>Customer Service Representative Are you ready ...</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>42.87111</td>\n",
       "      <td>-97.39728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Customer, Service, Representative, Are, you,...</td>\n",
       "      <td>[[customer, service, representative, are, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>331404</td>\n",
       "      <td>Kforce</td>\n",
       "      <td>Kforce has a client in Beaverton, Oregon (OR) ...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>45.48706</td>\n",
       "      <td>-122.80371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Kforce, has, a, client, in, Beaverton, ,, Or...</td>\n",
       "      <td>[[kforce, has, a, client, in, beaverton, orego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>322250</td>\n",
       "      <td>Kforce</td>\n",
       "      <td>Kforce has a client in Stamford, Connecticut (...</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>41.05343</td>\n",
       "      <td>-73.53873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Kforce, has, a, client, in, Stamford, ,, Con...</td>\n",
       "      <td>[[kforce, has, a, client, in, stamford, connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>157791</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>Here at Golden LivingCenters, we rely on and t...</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>40.67667</td>\n",
       "      <td>-95.85917</td>\n",
       "      <td>High school diploma or equivalent;Must within ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Here, at, Golden, LivingCenters, ,, we, rely...</td>\n",
       "      <td>[[here, at, golden, livingcenters, we, rely, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>119835</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>\"*Description* Description: SUMMARY At GameSto...</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.68149</td>\n",
       "      <td>-73.39984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[``, *Description*, Description, :, SUMMARY, ...</td>\n",
       "      <td>[[description, summary, at, gamestop, we, refe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>285609</td>\n",
       "      <td>Jewel-Osco</td>\n",
       "      <td>\"Updated 6/2011 JOB TITLE: Service Clerk (Bagg...</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>41.66892</td>\n",
       "      <td>-87.73866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provides prompt, efficient and friendly custom...</td>\n",
       "      <td>[[``, Updated, 6/2011, JOB, TITLE, :, Service,...</td>\n",
       "      <td>[[updated, job, title, service, clerk, bagger,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>89260</td>\n",
       "      <td>Express Scripts</td>\n",
       "      <td>\"Schedule: Full-time Job ID: 1500071I The Sale...</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>38.62727</td>\n",
       "      <td>-90.19789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o Proactive management of Houston entry &amp; comp...</td>\n",
       "      <td>[[``, Schedule, :, Full-time, Job, ID, :, 1500...</td>\n",
       "      <td>[[schedule, job, id, the, sales, coordinator, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>280096</td>\n",
       "      <td>JCPenney</td>\n",
       "      <td>Temp Support Specialist- Mall of Louisiana Loc...</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>30.45075</td>\n",
       "      <td>-91.15455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Executes the merchandise strategy You take the...</td>\n",
       "      <td>[[Temp, Support, Specialist-, Mall, of, Louisi...</td>\n",
       "      <td>[[temp, support, mall, of, louisiana, location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 hiringOrganization_organizationName  \\\n",
       "0       158844       Golfsmith International, Inc.   \n",
       "1       257645                               Intel   \n",
       "2       107875                    Florida Hospital   \n",
       "3       202394                Hitachi Data Systems   \n",
       "4       109675           Footprint Retail Services   \n",
       "5       215973                          Home Depot   \n",
       "6       207524                          Home Depot   \n",
       "7        64426                      East West Bank   \n",
       "8       245192                                 IBM   \n",
       "9       202429                Hitachi Data Systems   \n",
       "10      269503             J&J Family of Companies   \n",
       "11      139164                  Genesis Healthcare   \n",
       "12      255915                      Ingersoll Rand   \n",
       "13      173294         HamiltonConstructionCompany   \n",
       "14      116855                      G6 Hospitality   \n",
       "15       40701                         Dollar Tree   \n",
       "16      292406              Johns Hopkins Medicine   \n",
       "17      358904                           LHC Group   \n",
       "18      126507                                  GE   \n",
       "19       90538                                  EY   \n",
       "20       59256             Dr Pepper Snapple Group   \n",
       "21       85716     Eurofins Lancaster Laboratories   \n",
       "22      203342                             HMSHOST   \n",
       "23      283327                          Jewel-Osco   \n",
       "24      320876       KeyPoint Government Solutions   \n",
       "25      311069                      Kelly Services   \n",
       "26      141835              Genesis Rehabilitation   \n",
       "27      202406                Hitachi Data Systems   \n",
       "28      111597     Franciscan St. Eilzabeth Health   \n",
       "29      260448                               Intel   \n",
       "..         ...                                 ...   \n",
       "70      197429                            Hill-Rom   \n",
       "71      359233                           LHC Group   \n",
       "72        5520                     DCS Corporation   \n",
       "73      205825                          Home Depot   \n",
       "74       68546    Education Corporation of America   \n",
       "75      141067              Genesis Rehabilitation   \n",
       "76       26925        Destination Hotels & Resorts   \n",
       "77       81637                Epic Health Services   \n",
       "78      191317  Hewlett Packard Enterprise Company   \n",
       "79      134395                  Genesis Healthcare   \n",
       "80      177803                  Harris Corporation   \n",
       "81      222835                          Home Depot   \n",
       "82       12790                            Deloitte   \n",
       "83      319987            Kettering Medical Center   \n",
       "84      352211                 Learning Care Group   \n",
       "85      233278                              Humana   \n",
       "86      338881                 Kronos Incorporated   \n",
       "87       53773                       Dominos Pizza   \n",
       "88      158203                       Golden Living   \n",
       "89      156994                       Golden Living   \n",
       "90      249542                   ICF International   \n",
       "91      173219                     Hallmark Health   \n",
       "92       50969                       Dominos Pizza   \n",
       "93      331404                              Kforce   \n",
       "94      322250                              Kforce   \n",
       "95      157791                       Golden Living   \n",
       "96      119835                            GameStop   \n",
       "97      285609                          Jewel-Osco   \n",
       "98       89260                     Express Scripts   \n",
       "99      280096                            JCPenney   \n",
       "\n",
       "                                       jobDescription  \\\n",
       "0   \"Sales Associate Tracking Code 220425-971 Job ...   \n",
       "1   For PHY system engineering team within the Wir...   \n",
       "2   *RN Medical Oncology PCU Orlando - Nights* Flo...   \n",
       "3   Title: Specialist Sales Account Representative...   \n",
       "4   **Footprint Retail Services** **Job Descriptio...   \n",
       "5   Position Purpose: Provide outstanding service ...   \n",
       "6   The Asset Protection Specialist is primarily r...   \n",
       "7   # Job Description East West Bank is one of the...   \n",
       "8   Job Description IBM is seeking to hire a Senio...   \n",
       "9   Title: Field Solutions Engineer Location: New ...   \n",
       "10  Project Manager (m/w) - Government & Public Af...   \n",
       "11  Certified Medicine Aide Area of Interest: Nurs...   \n",
       "12  **Description:** At Ingersoll Rand we're passi...   \n",
       "13  \"Hamilton Construction,in businesssince 1939 a...   \n",
       "14  Business Unit: DirectEmployers Title: Manager ...   \n",
       "15  Auto req ID 41872BR Title ASSISTANT MANAGER Em...   \n",
       "16  Johns Hopkins employs more than 20,000 people ...   \n",
       "17  Job Description As a CNA, you will perform a v...   \n",
       "18  2306307 **Business** GE Capital **Business Seg...   \n",
       "19  Title: Greater China TAS Referrals (China Sout...   \n",
       "20  Merchandiser The Merchandiser is responsible f...   \n",
       "21  ### Eurofins is the world leader in the food, ...   \n",
       "22  \"**Having the right ingredients makes all the ...   \n",
       "23  JOB: Retail Clerk  Liquor/Beer/Wine JOB OVERVI...   \n",
       "24  **Overview:** KeyPoint Government Solutions is...   \n",
       "25  **Strategic Sales Lead  Premier Brands and Con...   \n",
       "26  Job Title: Occupational Therapist Area of Inte...   \n",
       "27  Title: Pentaho-Sales Executive-Texas Location:...   \n",
       "28  Title: Respiratory Therapy, Intern Location: X...   \n",
       "29  The Influencer Sales Group Solution Architect ...   \n",
       "..                                                ...   \n",
       "70  Title: Contract Svc Admin Location: United Sta...   \n",
       "71  Job Description The Occupational Therapist is ...   \n",
       "72  Systems Engineer - SE2-3678 Location: AL, Hunt...   \n",
       "73  Position Purpose:Associates in Office/Store Su...   \n",
       "74  # Description Education Corporation of America...   \n",
       "75  Job Title: Physical Therapist Area of Interest...   \n",
       "76  Busser Property Paradise Point Resort & Spa Co...   \n",
       "77  Epic Health Services, Inc. is looking for Bili...   \n",
       "78  Title: Software Engineer Location: China-Shang...   \n",
       "79  Registered Nurse Area of Interest: Nursing - R...   \n",
       "80  Description: Job Title: Electrical Engineer &n...   \n",
       "81  \"Position Purpose: Cashiers play a critical cu...   \n",
       "82  Deloitte is one of the leading professional se...   \n",
       "83  Description Greets patients and visitors in a ...   \n",
       "84  # Job Description Join our talented team, wher...   \n",
       "85  Role: Technology Architect Assignment: IT Loca...   \n",
       "86  * Observes and reports activities and incident...   \n",
       "87  \"ABOUT THE JOB Do you know why Domino's Pizza ...   \n",
       "88  Registered Dietician at AseraCare Hospice work...   \n",
       "89  At Golden LivingCenters, we care for every pat...   \n",
       "90  Energy Efficiency and Low Income Representativ...   \n",
       "91  Office Coordinator Geriatric Assessment Center...   \n",
       "92  Customer Service Representative Are you ready ...   \n",
       "93  Kforce has a client in Beaverton, Oregon (OR) ...   \n",
       "94  Kforce has a client in Stamford, Connecticut (...   \n",
       "95  Here at Golden LivingCenters, we rely on and t...   \n",
       "96  \"*Description* Description: SUMMARY At GameSto...   \n",
       "97  \"Updated 6/2011 JOB TITLE: Service Clerk (Bagg...   \n",
       "98  \"Schedule: Full-time Job ID: 1500071I The Sale...   \n",
       "99  Temp Support Specialist- Mall of Louisiana Loc...   \n",
       "\n",
       "   jobLocation_address_region  jobLocation_geo_latitude  \\\n",
       "0                  California                  33.91918   \n",
       "1                         NaN                       NaN   \n",
       "2                     Florida                  28.53834   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "5                     Indiana                  41.13060   \n",
       "6                  New Jersey                  40.21455   \n",
       "7                  California                  34.06862   \n",
       "8                         NaN                       NaN   \n",
       "9                         NaN                       NaN   \n",
       "10                        NaN                       NaN   \n",
       "11                     Kansas                  39.10972   \n",
       "12                  Tennessee                  35.14953   \n",
       "13                     Oregon                  44.04624   \n",
       "14                      Texas                  32.83707   \n",
       "15                      Texas                  32.85791   \n",
       "16                   Maryland                  39.29038   \n",
       "17                    Alabama                  34.74481   \n",
       "18                        NaN                       NaN   \n",
       "19                        NaN                       NaN   \n",
       "20                      Texas                  29.76328   \n",
       "21                       Ohio                  39.55895   \n",
       "22                        NaN                       NaN   \n",
       "23                    Montana                  48.19579   \n",
       "24                        NaN                       NaN   \n",
       "25                        NaN                       NaN   \n",
       "26                        NaN                       NaN   \n",
       "27                        NaN                       NaN   \n",
       "28                        NaN                       NaN   \n",
       "29                     Oregon                  45.52289   \n",
       "..                        ...                       ...   \n",
       "70                   Michigan                  42.78920   \n",
       "71                   Illinois                  41.51337   \n",
       "72                    Alabama                  34.73037   \n",
       "73              Massachusetts                  42.12509   \n",
       "74                    Alabama                  33.52066   \n",
       "75              New Hampshire                  42.99564   \n",
       "76                 California                  32.71533   \n",
       "77                 New Jersey                  39.95373   \n",
       "78                        NaN                       NaN   \n",
       "79              Massachusetts                  42.57509   \n",
       "80                    Florida                  28.03446   \n",
       "81                 New Mexico                  36.72806   \n",
       "82               Pennsylvania                  39.95234   \n",
       "83                       Ohio                  39.68950   \n",
       "84                    Arizona                  33.42227   \n",
       "85                   Kentucky                  38.25424   \n",
       "86                  Tennessee                  36.32311   \n",
       "87                      Texas                  29.76328   \n",
       "88                      Texas                  29.76328   \n",
       "89               South Dakota                  44.08054   \n",
       "90                   Michigan                  42.27756   \n",
       "91              Massachusetts                  42.41843   \n",
       "92               South Dakota                  42.87111   \n",
       "93                     Oregon                  45.48706   \n",
       "94                Connecticut                  41.05343   \n",
       "95                   Nebraska                  40.67667   \n",
       "96                   New York                  40.68149   \n",
       "97                   Illinois                  41.66892   \n",
       "98                   Missouri                  38.62727   \n",
       "99                  Louisiana                  30.45075   \n",
       "\n",
       "    jobLocation_geo_longitude  \\\n",
       "0                  -118.41647   \n",
       "1                         NaN   \n",
       "2                   -81.37924   \n",
       "3                         NaN   \n",
       "4                         NaN   \n",
       "5                   -85.12886   \n",
       "6                   -74.61932   \n",
       "7                  -118.02757   \n",
       "8                         NaN   \n",
       "9                         NaN   \n",
       "10                        NaN   \n",
       "11                  -95.08775   \n",
       "12                  -90.04898   \n",
       "13                 -123.02203   \n",
       "14                  -97.08195   \n",
       "15                  -97.25474   \n",
       "16                  -76.61219   \n",
       "17                  -87.66753   \n",
       "18                        NaN   \n",
       "19                        NaN   \n",
       "20                  -95.36327   \n",
       "21                  -84.30411   \n",
       "22                        NaN   \n",
       "23                 -114.31291   \n",
       "24                        NaN   \n",
       "25                        NaN   \n",
       "26                        NaN   \n",
       "27                        NaN   \n",
       "28                        NaN   \n",
       "29                 -122.98983   \n",
       "..                        ...   \n",
       "70                  -85.51669   \n",
       "71                  -87.67421   \n",
       "72                  -86.58610   \n",
       "73                  -72.74954   \n",
       "74                  -86.80249   \n",
       "75                  -71.45479   \n",
       "76                 -117.15726   \n",
       "77                  -74.19792   \n",
       "78                        NaN   \n",
       "79                  -70.93005   \n",
       "80                  -80.58866   \n",
       "81                 -108.21869   \n",
       "82                  -75.16379   \n",
       "83                  -84.16883   \n",
       "84                 -111.82264   \n",
       "85                  -85.75941   \n",
       "86                  -86.71333   \n",
       "87                  -95.36327   \n",
       "88                  -95.36327   \n",
       "89                 -103.23101   \n",
       "90                  -83.74088   \n",
       "91                  -71.10616   \n",
       "92                  -97.39728   \n",
       "93                 -122.80371   \n",
       "94                  -73.53873   \n",
       "95                  -95.85917   \n",
       "96                  -73.39984   \n",
       "97                  -87.73866   \n",
       "98                  -90.19789   \n",
       "99                  -91.15455   \n",
       "\n",
       "                                       qualifications  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   Must be eighteen years of age or older. Must p...   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12  Considerable knowledge of industry related sup...   \n",
       "13                                                NaN   \n",
       "14  High School diploma or equivalent;Computer pro...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  Doswiadczenie w pracy na podobnym stanowisku;W...   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25  Bachelors degree (MBA preferred) or equivalent...   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "..                                                ...   \n",
       "70                                                NaN   \n",
       "71                                                NaN   \n",
       "72                                                NaN   \n",
       "73                                                NaN   \n",
       "74                                                NaN   \n",
       "75                                                NaN   \n",
       "76                                                NaN   \n",
       "77                                                NaN   \n",
       "78                                                NaN   \n",
       "79  Registered Nurse / RN Requirements:;Bachelor's...   \n",
       "80  Master&rsquo s degree in electrical engineerin...   \n",
       "81                                                NaN   \n",
       "82  Establishes deliverable structure and content ...   \n",
       "83                                                NaN   \n",
       "84                                                NaN   \n",
       "85                                                NaN   \n",
       "86                                                NaN   \n",
       "87  General job duties for all store team members;...   \n",
       "88                                                NaN   \n",
       "89  Currently licensed or registered in state of p...   \n",
       "90                                            / /;/ /   \n",
       "91                                                NaN   \n",
       "92                                                NaN   \n",
       "93                                                NaN   \n",
       "94                                                NaN   \n",
       "95  High school diploma or equivalent;Must within ...   \n",
       "96                                                NaN   \n",
       "97                                                NaN   \n",
       "98                                                NaN   \n",
       "99                                                NaN   \n",
       "\n",
       "                                     responsibilities  \\\n",
       "0   \"Ensure each Customer receives exceptional ser...   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4   A Merchandiser must complete all assigned merc...   \n",
       "5   Provide outstanding service to ensure efficien...   \n",
       "6                                                 NaN   \n",
       "7   We are currently seeking a Customer Service Ce...   \n",
       "8                                                 NaN   \n",
       "9   Job Functions;Specific duties in this role wil...   \n",
       "10                                                NaN   \n",
       "11  Receives report at the beginning of shift with...   \n",
       "12  Ingersoll Rand is a diverse and inclusive envi...   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17  Your specific duties for this role will includ...   \n",
       "18  Prowadzenie projektow zw\\. z rozwojem pracowni...   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22  \"Greets customers and takes food order provide...   \n",
       "23  Sells, receives, stocks and may order beer/liq...   \n",
       "24                                                NaN   \n",
       "25  This is a field based corporate sales position...   \n",
       "26  Screens, examines and evaluates patients, incl...   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "..                                                ...   \n",
       "70                   _ Other duties may be assigned:_   \n",
       "71                                                NaN   \n",
       "72  Participate in preparation of technical and pr...   \n",
       "73  Usually in a comfortable environment but with ...   \n",
       "74  Education Corporation of America owns and oper...   \n",
       "75  Genesis Rehabilitation Services is looking for...   \n",
       "76                                                NaN   \n",
       "77  Provide skilled nursing care to pediatric and ...   \n",
       "78                                                NaN   \n",
       "79  Twin Oaks in Danvers MA is looking for a Per D...   \n",
       "80                                                NaN   \n",
       "81  Usually in a comfortable environment but with ...   \n",
       "82  Job Responsibilities;In providing finance-rela...   \n",
       "83                                                NaN   \n",
       "84  Join our talented team, where we inspire child...   \n",
       "85  Guide all aspects of design, implementation an...   \n",
       "86                                                NaN   \n",
       "87  \"You must be 18 years of age and have a valid ...   \n",
       "88                                                NaN   \n",
       "89                                                NaN   \n",
       "90                                                NaN   \n",
       "91  High School Diploma or equivalent. Bachelor's ...   \n",
       "92                                                NaN   \n",
       "93                                                NaN   \n",
       "94                                                NaN   \n",
       "95                                                NaN   \n",
       "96                                                NaN   \n",
       "97  Provides prompt, efficient and friendly custom...   \n",
       "98  o Proactive management of Houston entry & comp...   \n",
       "99  Executes the merchandise strategy You take the...   \n",
       "\n",
       "                                      tokenized_sents  \\\n",
       "0   [[``, Sales, Associate, Tracking, Code, 220425...   \n",
       "1   [[For, PHY, system, engineering, team, within,...   \n",
       "2   [[*RN, Medical, Oncology, PCU, Orlando, -, Nig...   \n",
       "3   [[Title, :, Specialist, Sales, Account, Repres...   \n",
       "4   [[**Footprint, Retail, Services**, **Job, Desc...   \n",
       "5   [[Position, Purpose, :, Provide, outstanding, ...   \n",
       "6   [[The, Asset, Protection, Specialist, is, prim...   \n",
       "7   [[#, Job, Description, East, West, Bank, is, o...   \n",
       "8   [[Job, Description, IBM, is, seeking, to, hire...   \n",
       "9   [[Title, :, Field, Solutions, Engineer, Locati...   \n",
       "10  [[Project, Manager, (, m/w, ), -, Government, ...   \n",
       "11  [[Certified, Medicine, Aide, Area, of, Interes...   \n",
       "12  [[**Description, :, **, At, Ingersoll, Rand, w...   \n",
       "13  [[``, Hamilton, Construction, ,, in, businesss...   \n",
       "14  [[Business, Unit, :, DirectEmployers, Title, :...   \n",
       "15  [[Auto, req, ID, 41872BR, Title, ASSISTANT, MA...   \n",
       "16  [[Johns, Hopkins, employs, more, than, 20,000,...   \n",
       "17  [[Job, Description, As, a, CNA, ,, you, will, ...   \n",
       "18  [[2306307, **Business**, GE, Capital, **Busine...   \n",
       "19  [[Title, :, Greater, China, TAS, Referrals, (,...   \n",
       "20  [[Merchandiser, The, Merchandiser, is, respons...   \n",
       "21  [[#, #, #, Eurofins, is, the, world, leader, i...   \n",
       "22  [[``, **Having, the, right, ingredients, makes...   \n",
       "23  [[JOB, :, Retail, Clerk, Liquor/Beer/Wine, JOB...   \n",
       "24  [[**Overview, :, **, KeyPoint, Government, Sol...   \n",
       "25  [[**Strategic, Sales, Lead, Premier, Brands, a...   \n",
       "26  [[Job, Title, :, Occupational, Therapist, Area...   \n",
       "27  [[Title, :, Pentaho-Sales, Executive-Texas, Lo...   \n",
       "28  [[Title, :, Respiratory, Therapy, ,, Intern, L...   \n",
       "29  [[The, Influencer, Sales, Group, Solution, Arc...   \n",
       "..                                                ...   \n",
       "70  [[Title, :, Contract, Svc, Admin, Location, :,...   \n",
       "71  [[Job, Description, The, Occupational, Therapi...   \n",
       "72  [[Systems, Engineer, -, SE2-3678, Location, :,...   \n",
       "73  [[Position, Purpose, :, Associates, in, Office...   \n",
       "74  [[#, Description, Education, Corporation, of, ...   \n",
       "75  [[Job, Title, :, Physical, Therapist, Area, of...   \n",
       "76  [[Busser, Property, Paradise, Point, Resort, &...   \n",
       "77  [[Epic, Health, Services, ,, Inc., is, looking...   \n",
       "78  [[Title, :, Software, Engineer, Location, :, C...   \n",
       "79  [[Registered, Nurse, Area, of, Interest, :, Nu...   \n",
       "80  [[Description, :, Job, Title, :, Electrical, E...   \n",
       "81  [[``, Position, Purpose, :, Cashiers, play, a,...   \n",
       "82  [[Deloitte, is, one, of, the, leading, profess...   \n",
       "83  [[Description, Greets, patients, and, visitors...   \n",
       "84  [[#, Job, Description, Join, our, talented, te...   \n",
       "85  [[Role, :, Technology, Architect, Assignment, ...   \n",
       "86  [[*, Observes, and, reports, activities, and, ...   \n",
       "87  [[``, ABOUT, THE, JOB, Do, you, know, why, Dom...   \n",
       "88  [[Registered, Dietician, at, AseraCare, Hospic...   \n",
       "89  [[At, Golden, LivingCenters, ,, we, care, for,...   \n",
       "90  [[Energy, Efficiency, and, Low, Income, Repres...   \n",
       "91  [[Office, Coordinator, Geriatric, Assessment, ...   \n",
       "92  [[Customer, Service, Representative, Are, you,...   \n",
       "93  [[Kforce, has, a, client, in, Beaverton, ,, Or...   \n",
       "94  [[Kforce, has, a, client, in, Stamford, ,, Con...   \n",
       "95  [[Here, at, Golden, LivingCenters, ,, we, rely...   \n",
       "96  [[``, *Description*, Description, :, SUMMARY, ...   \n",
       "97  [[``, Updated, 6/2011, JOB, TITLE, :, Service,...   \n",
       "98  [[``, Schedule, :, Full-time, Job, ID, :, 1500...   \n",
       "99  [[Temp, Support, Specialist-, Mall, of, Louisi...   \n",
       "\n",
       "                                     normalized_sents  \n",
       "0   [[sales, associate, tracking, code, job, descr...  \n",
       "1   [[for, phy, system, engineering, team, within,...  \n",
       "2   [[medical, oncology, pcu, orlando, florida, ho...  \n",
       "3   [[title, specialist, sales, account, represent...  \n",
       "4   [[retail, job, title, retail, merchandiser, re...  \n",
       "5   [[position, purpose, provide, outstanding, ser...  \n",
       "6   [[the, asset, protection, specialist, is, prim...  \n",
       "7   [[job, description, east, west, bank, is, one,...  \n",
       "8   [[job, description, ibm, is, seeking, to, hire...  \n",
       "9   [[title, field, solutions, engineer, location,...  \n",
       "10  [[project, manager, government, public, jansse...  \n",
       "11  [[certified, medicine, aide, area, of, interes...  \n",
       "12  [[at, ingersoll, rand, we, passionate, about, ...  \n",
       "13  [[hamilton, construction, in, businesssince, a...  \n",
       "14  [[business, unit, directemployers, title, mana...  \n",
       "15  [[auto, req, id, title, assistant, manager, em...  \n",
       "16  [[johns, hopkins, employs, more, than, people,...  \n",
       "17  [[job, description, as, a, cna, you, will, per...  \n",
       "18  [[ge, capital, capital, international, bank, b...  \n",
       "19  [[title, greater, china, tas, referrals, china...  \n",
       "20  [[merchandiser, the, merchandiser, is, respons...  \n",
       "21  [[eurofins, is, the, world, leader, in, the, f...  \n",
       "22  [[the, right, ingredients, makes, all, the, di...  \n",
       "23  [[job, retail, clerk, job, overview, provides,...  \n",
       "24  [[keypoint, government, solutions, is, current...  \n",
       "25  [[sales, lead, premier, brands, and, consumer,...  \n",
       "26  [[job, title, occupational, therapist, area, o...  \n",
       "27  [[title, location, texas, the, regional, accou...  \n",
       "28  [[title, respiratory, therapy, intern, location]]  \n",
       "29  [[the, influencer, sales, group, solution, arc...  \n",
       "..                                                ...  \n",
       "70  [[title, contract, svc, admin, location, unite...  \n",
       "71  [[job, description, the, occupational, therapi...  \n",
       "72  [[systems, engineer, location, al, huntsville,...  \n",
       "73  [[position, purpose, associates, in, support, ...  \n",
       "74  [[description, education, corporation, of, ame...  \n",
       "75  [[job, title, physical, therapist, area, of, i...  \n",
       "76  [[busser, property, paradise, point, resort, s...  \n",
       "77  [[epic, health, services, is, looking, for, bi...  \n",
       "78  [[title, software, engineer, location, other, ...  \n",
       "79  [[registered, nurse, area, of, interest, nursi...  \n",
       "80  [[description, job, title, electrical, enginee...  \n",
       "81  [[position, purpose, cashiers, play, a, critic...  \n",
       "82  [[deloitte, is, one, of, the, leading, profess...  \n",
       "83  [[description, greets, patients, and, visitors...  \n",
       "84  [[job, description, join, our, talented, team,...  \n",
       "85  [[role, technology, architect, assignment, it,...  \n",
       "86  [[observes, and, reports, activities, and, inc...  \n",
       "87  [[about, the, job, do, you, know, why, domino,...  \n",
       "88  [[registered, dietician, at, aseracare, hospic...  \n",
       "89  [[at, golden, livingcenters, we, care, for, ev...  \n",
       "90  [[energy, efficiency, and, low, income, repres...  \n",
       "91  [[office, coordinator, geriatric, assessment, ...  \n",
       "92  [[customer, service, representative, are, you,...  \n",
       "93  [[kforce, has, a, client, in, beaverton, orego...  \n",
       "94  [[kforce, has, a, client, in, stamford, connec...  \n",
       "95  [[here, at, golden, livingcenters, we, rely, o...  \n",
       "96  [[description, summary, at, gamestop, we, refe...  \n",
       "97  [[updated, job, title, service, clerk, bagger,...  \n",
       "98  [[schedule, job, id, the, sales, coordinator, ...  \n",
       "99  [[temp, support, mall, of, louisiana, location...  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDF = pandas.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Engineering including below jobs: 1. Hardware designing of DCS 2. Software configurations, programming, testing of DCS/PLC 3. Testing and FAT 4. Installation and commissioning. 5. Material ordering, approvals of datasheets. 6. HSE compliance as per HSE directives of HON. BE / B. Tech - Instrumentation / Control / Electronics. **Job:** **Engineering* **Title:** *Systems Engineer* **Location:** *IND-MH-Pune* **Requisition ID:** *00302235*\n",
      "\n",
      "\n",
      "Like talking on the phone? Enjoy giving great customer service? Use those skills while working flexible,part time hours.\n",
      "\n",
      "\n",
      "*# Positions:* 2 *Location:* US - UT - Orem *Category:* Engineering\n",
      "\n",
      "\n",
      "Title: Respiratory Therapy, Intern Location: XX-XX-XX\n",
      "\n",
      "\n",
      "Title: Position Opening at Illinois Wesleyan University Location: US-IL-Bloomington\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adprob([[\"basic\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1a*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 1a or 1b.** Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> My dataset contains the most popular 1000 novels. For this exercise, I look for the least popular novels. I scraped the second last page of the male and female ranking board to make sure that there are 50 novels on each of the page (so it is more structured).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[['//book.qidian.com/info/1003683686'], ['//book.qidian.com/info/1003690577'], ['//book.qidian.com/info/1003692354'], ['//book.qidian.com/info/1003692242'], ['//book.qidian.com/info/1003708434'], ['//book.qidian.com/info/1003707566'], ['//book.qidian.com/info/1003710138'], ['//book.qidian.com/info/1003712764'], ['//book.qidian.com/info/1003713093'], ['//book.qidian.com/info/1003713099']]\n",
      "50\n",
      "[['//book.qidian.com/info/1004120556'], ['//book.qidian.com/info/1004120722'], ['//book.qidian.com/info/1004120871'], ['//book.qidian.com/info/1004121114'], ['//book.qidian.com/info/1004121151'], ['//book.qidian.com/info/1004121656'], ['//book.qidian.com/info/1004122213'], ['//book.qidian.com/info/1004122422'], ['//book.qidian.com/info/1004122571'], ['//book.qidian.com/info/1004124134']]\n"
     ]
    }
   ],
   "source": [
    "#Scraping more books \n",
    "BookList_male= []\n",
    "\n",
    "#I take the second last page to make sure that there are 50 books on the page\n",
    "rank_url= \"https://www.qidian.com/all?size=2&orderId=&style=2&pageSize=50&siteid=1&pubflag=0&hiddenField=0&page=280\"\n",
    "\n",
    "page_request= requests.get(rank_url)\n",
    "page_soup= BeautifulSoup(page_request.text, \"lxml\")\n",
    "\n",
    "url_list= page_soup.body.find(\"div\", attrs= {\"class\": \"all-book-list\"})\\\n",
    "           .find_all(\"a\", attrs= {\"class\": \"name\"}) #Continuing the previous line!!!\n",
    "for i in range(50): \n",
    "    a_book= [url_list[i][\"href\"]]\n",
    "    BookList_male.append(a_book)\n",
    "\n",
    "#Output of this loop is links of the books on the ranking broad.\n",
    "#Output is list within list\n",
    "#[[book1's url, number of being bookmarked], [book2's url, number of being bookmarked], etc]\n",
    "print(len(BookList_male))\n",
    "print(BookList_male[:10])\n",
    "\n",
    "#=====================================================================================\n",
    "BookList_female= []\n",
    "\n",
    "#I take the second last page to make sure that there are 50 books on the page\n",
    "rank_url= \"https://www.qidian.com/mm/all?size=2&orderId=&style=2&pageSize=50&siteid=0&pubflag=0&hiddenField=0&page=195\"\n",
    "\n",
    "page_request= requests.get(rank_url)\n",
    "page_soup= BeautifulSoup(page_request.text, \"lxml\")\n",
    "\n",
    "url_list= page_soup.body.find(\"div\", attrs= {\"class\": \"all-book-list\"})\\\n",
    "           .find_all(\"a\", attrs= {\"class\": \"name\"}) #Continuing the previous line!!!\n",
    "for i in range(50): \n",
    "    a_book= [url_list[i][\"href\"]]\n",
    "    BookList_female.append(a_book)\n",
    "\n",
    "#Output of this loop is links of the books on the ranking broad.\n",
    "#Output is list within list\n",
    "#[[book1's url, number of being bookmarked], [book2's url, number of being bookmarked], etc]\n",
    "print(len(BookList_female))\n",
    "print(BookList_female[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soup_a_book(a_book_url):\n",
    "    ##Extracting book introduction from a webpage+ Using Beautifulsoup to parse a book's web page\n",
    "    a_book_request= requests.get(a_book_url)\n",
    "    a_book_soup= BeautifulSoup(a_book_request.text, 'lxml')\n",
    "    return a_book_soup\n",
    "\n",
    "\n",
    "\n",
    "def get_text(a_book_soup):\n",
    "    #Input is the soup of the introduction page of the book \n",
    "    #Obtain 10 chapters as example writing, 10 chapters are stored in one dictionary\n",
    "    \n",
    "    #Using the introduction page to get the the \"read for free (= first page)\"\n",
    "    def get_content(for_read_url):\n",
    "        read_url= \"https:\"+ for_read_url\n",
    "\n",
    "        ch= requests.get(read_url)\n",
    "        ch_soup= BeautifulSoup(ch.text, 'lxml')\n",
    "        #Strange!!! I followed the teacher's code and use \"html.parser\",\n",
    "        #and I get a TypeError called:\n",
    "        #TypeError: 'NoneType' object is not subscriptable\n",
    "        #This problem is no longer there, if I somehow use \"lxml\"\n",
    "        #I find this out completely by accident! Pure luck!\n",
    "\n",
    "        chP= ch_soup.body.find(\"div\", attrs= {\"class\": \"read-content\"}).find_all(\"p\")\n",
    "        chP_joined= \"\"\n",
    "        for p in chP:\n",
    "            chP_joined+= str(p)   \n",
    "        chP_joined= chP_joined.replace(\"\\u3000\", \"\")\n",
    "        #print(for_chP)\n",
    "        ch_content= re.sub(r\"(<\\/*p>)(\\1*)\", \"\", chP_joined) #(Regular Expression 5)\n",
    "        ch_content= ch_content.strip()\n",
    "        return ch_content, ch_soup\n",
    "    \n",
    "    content_dic= {}\n",
    "    i= 1 \n",
    "    while i< 31:\n",
    "        if i== 1:\n",
    "            for_read_url= a_book_soup.body.find(\"a\", text= \"免费试读\")[\"href\"] \n",
    "            ch_content, ch_soup = get_content(for_read_url)\n",
    "            content_dic[i]= ch_content\n",
    "            i+= 1\n",
    "        else:\n",
    "            for_read_url= ch_soup.body.find(\"a\", text= \"下一章\")[\"href\"]\n",
    "            #print(i, for_read_url)\n",
    "            ch_content, ch_soup = get_content(for_read_url)\n",
    "            content_dic[i]= ch_content\n",
    "            i+= 1\n",
    "    \n",
    "    return content_dic\n",
    "\n",
    "def deal_with_a_book(a_book_url):\n",
    "    a_book_soup= soup_a_book(a_book_url)\n",
    "\n",
    "    content_dic= get_text(a_book_soup)\n",
    "    \n",
    "    a_book_dic= {\"example_text\": content_dic}\n",
    "    helper= [a_book_dic]\n",
    "    bookDF= pd.DataFrame(data= helper)\n",
    "    return bookDF, a_book_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531.9532141685486\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_male= pd.DataFrame({})\n",
    "\n",
    "for book in BookList_male[:10]:\n",
    "    book_url= \"https:\"+ book[0]\n",
    "    a_bookDF, a_book_soup= deal_with_a_book(book_url)\n",
    "    #The \"number of being bookmarked\" information is not available on the\n",
    "    #book introduction page. This information is available on the book ranking page. \n",
    "    #Thus, I add a column for each book about its number of being bookmarked:\n",
    "\n",
    "    test_male= test_male.append(a_bookDF)\n",
    "    test_male.to_pickle(\"test_male.pickle\")\n",
    "        \n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528.1364879608154\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_female= pd.DataFrame({})\n",
    "\n",
    "for book in BookList_female[:10]:\n",
    "    book_url= \"https:\"+ book[0]\n",
    "    a_bookDF, a_book_soup= deal_with_a_book(book_url)\n",
    "    #The \"number of being bookmarked\" information is not available on the\n",
    "    #book introduction page. This information is available on the book ranking page. \n",
    "    #Thus, I add a column for each book about its number of being bookmarked:\n",
    "\n",
    "    test_female= test_female.append(a_bookDF)\n",
    "    test_female.to_pickle(\"test_female.pickle\")\n",
    "        \n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [回忆, ，, 如果, 带给, 人, 的, 是, 难以, 抹, 去, 的, 痛苦, ，, 这...\n",
       "0    [微风, 徐徐, 的, 吹, 过, 校园, ，, 我, 抬头, 望, 着, 天空, ，, 秋...\n",
       "0    [毛玉, ，, M, 市, 第一, 人民, 医院, 最, 年轻漂亮, 的, 副, 主任医师,...\n",
       "0    [落红, 还, 没, 完全, 化作, 春泥, ，, 石榴, 已有, 了, 笑意, 。, 接踵...\n",
       "0    [二十一, 世纪, ，, A, 市, 一个, 都市, 村庄, 内, ，, 叶, 清雅, 就,...\n",
       "0    [如果, 你, 要, 问, 最近, 有没有, 什么, 热门话题, ，, 那, 必定, 是, ...\n",
       "0    [身上, 很, 疼, ，, 尤其, 是, 脑袋, 。,  ,  , 周, 浅浅, 吧嗒, 着...\n",
       "0    [这个, 是, 什么, ？, 这个, 又, 是, 什么, ？, 这个, ！, “, 终于, ...\n",
       "0    [“, 现在, 可以, 说, 是, 怎么回事, 了, 吧, ？, ”, 飞儿, 坐在, 车里...\n",
       "0    [“, 雨笑, ，, 喂, …, …, ！, 雨笑, …, …, …, …, …, …, ”...\n",
       "Name: example_token, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_ch(thirty_ch):\n",
    "    #Take the 30-chapter dictionary as input\n",
    "    #Tokenize the 30 chapters, combine them and output a list \n",
    "    thirty_ch_list= []\n",
    "    for ch in thirty_ch.keys():\n",
    "        ch_list= jieba.lcut(thirty_ch[ch], cut_all= False)\n",
    "        thirty_ch_list+= ch_list\n",
    "    return thirty_ch_list\n",
    "\n",
    "\n",
    "test_male[\"example_token\"]= test_male[\"example_text\"].apply(lambda x: tokenize_ch(x))\n",
    "test_male[\"example_token\"]\n",
    "\n",
    "test_female[\"example_token\"]= test_female[\"example_text\"].apply(lambda x: tokenize_ch(x))\n",
    "test_female[\"example_token\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> I did not save the stop words, and I really do not want to run everything again just to construct the stop word list. So I decide to just copy and paste them here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words= ['$', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '_', '“', \n",
    "             '”', '、', '。', '《', '》', '一', '一些', '一何', '一则', '一方面', '一旦', \n",
    "             '一来', '一样', '一般', '万一', '上', '上下', '下', '不', '不仅', '不但', '不光', \n",
    "             '不单', '不只', '不外乎', '不如', '不妨', '不尽', '不尽然', '不得', '不怕', '不惟', \n",
    "             '不成', '不拘', '不料', '不是', '不比', '不然', '不特', '不独', '不管', '不至于', \n",
    "             '不若', '不论', '不过', '不问', '与', '与其', '与其说', '与否', '与此同时', '且', \n",
    "             '且不说', '且说', '两者', '个', '个别', '临', '为', '为了', '为什么', '为何', '为止', \n",
    "             '为此', '为着', '乃', '乃至', '乃至于', '么', '之', '之一', '之所以', '之类', '乌乎', \n",
    "             '乎', '乘', '也', '也好', '也罢', '了', '二来', '于', '于是', '于是乎', '云云', '云尔', \n",
    "             '些', '亦', '人', '人们', '人家', '什么', '什么样', '今', '介于', '仍', '仍旧', '从', \n",
    "             '从此', '从而', '他人', '以', '以上', '以为', '以便', '以免', '以及', '以故', '以期', \n",
    "             '以来', '以至', '以至于', '以致', '们', '任', '任何', '任凭', '似的', '但', '但凡', \n",
    "             '但是', '何', '何以', '何况', '何处', '何时', '余外', '作为', '你', '你们', '使', \n",
    "             '使得', '例如', '依', '依据', '依照', '便于', '俺', '俺们', '倘', '倘使', '倘或', \n",
    "             '倘然', '倘若', '借', '假使', '假如', '假若', '傥然', '像', '儿', '先不先', '光是', \n",
    "             '全体', '全部', '兮', '关于', '其', '其一', '其中', '其二', '其他', '其余', '其它', \n",
    "             '其次', '具体地说', '具体说来', '兼之', '内', '再', '再其次', '再则', '再有', '再者', \n",
    "             '再者说', '再说', '冒', '冲', '况且', '几', '几时', '凡', '凡是', '凭', '凭借', '出于', \n",
    "             '出来', '分别', '则', '则甚', '别', '别人', '别处', '别是', '别的', '别管', '别说', '到', \n",
    "             '前后', '前此', '前者', '加之', '加以', '即', '即令', '即使', '即便', '即如', '即或', \n",
    "             '即若', '却', '去', '又', '又及', '及', '及其', '及至', '反之', '反而', '反过来', \n",
    "             '反过来说', '受到', '另', '另一方面', '另外', '另悉', '只', '只当', '只怕', '只是', \n",
    "             '只有', '只消', '只要', '只限', '叫', '可', '可以', '可是', '可见', '各', '各个', \n",
    "             '各位', '各种', '各自', '同', '同时', '后', '后者', '向', '向使', '向着', '吓', '吗', \n",
    "             '否则', '吧', '吧哒', '吱', '呀', '呃', '呕', '呗', '呜', '呜呼', '呢', '呵', '呵呵', \n",
    "             '呸', '呼哧', '咋', '和', '咚', '咦', '咧', '咱', '咱们', '咳', '哇', '哈', '哈哈', '哉', \n",
    "             '哎', '哎呀', '哎哟', '哗', '哟', '哦', '哩', '哪', '哪个', '哪些', '哪儿', '哪天', \n",
    "             '哪年', '哪怕', '哪样', '哪边', '哪里', '哼', '哼唷', '唉', '唯有', '啊', '啐', '啥', \n",
    "             '啦', '啪达', '啷当', '喂', '喏', '喔唷', '喽', '嗡', '嗡嗡', '嗬', '嗯', '嗳', '嘎', \n",
    "             '嘎登', '嘘', '嘛', '嘻', '嘿', '嘿嘿', '因', '因为', '因了', '因此', '因着', '因而', \n",
    "             '固然', '在', '在下', '在于', '地', '基于', '处在', '多', '多么', '多少', '大', '大家', \n",
    "             '好', '如', '如上', '如上所述', '如下', '如何', '如其', '如同', '如是', '如果', '如此', \n",
    "             '如若', '始而', '孰料', '孰知', '宁', '宁可', '宁愿', '宁肯', '它', '它们', '对', '对于', \n",
    "             '对待', '对方', '对比', '将', '小', '尔', '尔后', '尔尔', '尚且', '就', '就是', '就是了', \n",
    "             '就是说', '就算', '就要', '尽', '尽管', '尽管如此', '岂但', '己', '已', '已矣', '巴', \n",
    "             '巴巴', '并', '并且', '并非', '庶乎', '庶几', '开外', '开始', '归', '归齐', '当', \n",
    "             '当地', '当然', '当着', '彼', '彼时', '彼此', '往', '待', '很', '得', '得了', '怎', \n",
    "             '怎么', '怎么办', '怎么样', '怎奈', '怎样', '总之', '总的来看', '总的来说', '总的说来', \n",
    "             '总而言之', '恰恰相反', '您', '惟其', '慢说', '我', '我们', '或', '或则', '或是', \n",
    "             '或曰', '或者', '截至', '所', '所以', '所在', '所幸', '所有', '才', '才能', '打', \n",
    "             '打从', '把', '抑或', '拿', '按', '按照', '换句话说', '换言之', '据', '据此', '接着', \n",
    "             '故', '故此', '故而', '旁人', '无', '无宁', '无论', '既', '既往', '既是', '既然', \n",
    "             '时候', '是', '是以', '是的', '曾', '替', '替代', '有', '有些', '有关', '有及', '有时', \n",
    "             '有的', '望', '朝', '朝着', '本', '本人', '本地', '本着', '本身', '来', '来着', '来自', \n",
    "             '来说', '极了', '果然', '果真', '某', '某个', '某些', '某某', '根据', '欤', '正值', '正如', \n",
    "             '正巧', '正是', '此', '此地', '此处', '此外', '此时', '此次', '此间', '毋宁', '每', '每当', \n",
    "             '比', '比及', '比如', '比方', '没奈何', '沿', '沿着', '漫说', '焉', '然则', '然后', '然而', \n",
    "             '照', '照着', '犹且', '犹自', '甚且', '甚么', '甚或', '甚而', '甚至', '甚至于', '用', '用来', \n",
    "             '由', '由于', '由是', '由此', '由此可见', '的', '的确', '的话', '直到', '相对而言', '省得', \n",
    "             '看', '眨眼', '着', '着呢', '矣', '矣乎', '矣哉', '离', '竟而', '第', '等', '等到', '等等', \n",
    "             '简言之', '管', '类如', '紧接着', '纵', '纵令', '纵使', '纵然', '经', '经过', \n",
    "             '结果', '给', '继之', '继后', '继而', '综上所述', '罢了', '者', '而', '而且', '而况', \n",
    "             '而后', '而外', '而已', '而是', '而言', '能', '能否', '腾', '自', '自个儿', '自从', \n",
    "             '自各儿', '自后', '自家', '自己', '自打', '自身', '至', '至于', '至今', '至若', '致', \n",
    "             '般的', '若', '若夫', '若是', '若果', '若非', '莫不然', '莫如', '莫若', '虽', '虽则', \n",
    "             '虽然', '虽说', '被', '要', '要不', '要不是', '要不然', '要么', '要是', '譬喻', '譬如', \n",
    "             '让', '许多', '论', '设使', '设或', '设若', '诚如', '诚然', '该', '说来', '诸', '诸位', \n",
    "             '诸如', '谁', '谁人', '谁料', '谁知', '贼死', '赖以', '赶', '起', '起见', '趁', '趁着', \n",
    "             '越是', '距', '跟', '较', '较之', '边', '过', '还', '还是', '还有', '还要', '这', \n",
    "             '这一来', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', \n",
    "             '这就是说', '这时', '这样', '这次', '这般', '这边', '这里', '进而', '连', '连同', \n",
    "             '逐步', '通过', '遵循', '遵照', '那', '那个', '那么', '那么些', '那么样', '那些', \n",
    "             '那会儿', '那儿', '那时', '那样', '那般', '那边', '那里', '都', '鄙人', '鉴于', '针对',\n",
    "             '阿', '除', '除了', '除外', '除开', '除此之外', '除非', '随', '随后', '随时', '随着', \n",
    "             '难道说', '非但', '非徒', '非特', '非独', '靠', '顺', '顺着', '首先', '！', '，', '：',\n",
    "             '；', '？'\n",
    "             '，', '。', ' ', '…', '《', '》', '！', '？', '‘', '’', '、', '：', ':', '２', \n",
    "             '（', '）', '—', '*', '；', '“', '”', '\\r', '!', '?', ',', '=', '【', '】', '~', \n",
    "             'V', '％', '-', 'ｃ', 'X', 'ｅ', '^', '_', '<', 'a', '\"', '/', '#', '>', 'Ｐ', 'Ｓ', \n",
    "             '＜', '＞', '+', '%', '～', '.', '##########', '[', ']', '＝', '·', 'ㄟ', '(', '▔', ')', \n",
    "             'ㄏ', '－', '2', 'ｍ', '@', '８', '６', '０', 'の', '剣', 'を', '喰', 'ら', 'え', '啾', \n",
    "             '|', '▍', '1', '╰', '☆', '＋', '∽', '\\xa0', '3', '0', '&#', '�', '╄', '\\\\', 'Ⅱ', \n",
    "             'u', '①', '②', '③', '④', '．', 'Ｙ', '{', '}', '###########################', '4', \n",
    "             '6', '7', '8', 'A', 'B', '&', '┃', 'ゞ', '「', '」', '狷', '→', ';', '※', \n",
    "             '+++++++++++++++++++++++++++++++++++++++++++++++', \n",
    "              '∝', '灞', '＊', 'Ｑ', '+++++++++++++++++++++++++++++++++++++']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopw(wordlist, stopw):\n",
    "    cleaned_list= []\n",
    "    for w in wordlist:\n",
    "        if w not in stopw:\n",
    "            cleaned_list+= [w]\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_male[\"example_token_clean\"]= test_male[\"example_token\"].apply(lambda x: remove_stopw(x, stop_words))\n",
    "test_female[\"example_token_clean\"]= test_female[\"example_token\"].apply(lambda x: remove_stopw(x, stop_words))\n",
    "test_DF= pd.concat([test_male[[\"example_token_clean\", \"example_text\"]], \n",
    "                    test_female[[\"example_token_clean\", \"example_text\"]]],\n",
    "                  axis= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> For the errors below... I think this exercise is not going to work for me right now.\n",
    "\n",
    "<span style=\"color:green\"> I do not understand the first error. The model of the example is also of the class  'gensim.models.word2vec.Word2Vec', so is my model. But then \"'Word2Vec' object has no attribute 'index2word'\" when it comes to my model. Confused...\n",
    "\n",
    "<span style=\"color:green\"> For the second error, I choose not to fix it. The error tells me to set hs= 1 in the model training for this operation to work. But Linzhuo noticed that there is some problems with the model when hs= 1: The training loss continuously increases rather than decreases as the training epoch increases. I do not want to set hs to 1 since I don't know what is wrong and do not trust the hs= 1 model. If the trained model is suspecious (by setting hs= 1), then it doesn't make sense to compare the new data with the original model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-70b0351b38ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbook_model\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../bookW2V_loss.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "book_model  = gensim.models.word2vec.Word2Vec.load('../bookW2V_loss.model')\n",
    "print(type(book_model))\n",
    "vocab = book_model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "#This is from the example above. \n",
    "#Do not understand why it works for the example but not for mine. \n",
    "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')\n",
    "print(type(resume_model))\n",
    "testing_vocab= resume_model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "We have currently only implemented score for the hierarchical softmax scheme, so you need to have run word2vec with hs=1 and negative=0 for this to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b5d96c9f03ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'likelihood'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'example_token_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-b5d96c9f03ed>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'likelihood'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'example_token_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-6782f974d8d7>\u001b[0m in \u001b[0;36madprob\u001b[0;34m(ad, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msen_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mad_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mad_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, sentences, total_sentences, chunksize, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             raise RuntimeError(\n\u001b[0;32m-> 1172\u001b[0;31m                 \u001b[0;34m\"We have currently only implemented score for the hierarchical softmax scheme, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m                 \u001b[0;34m\"so you need to have run word2vec with hs=1 and negative=0 for this to work.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: We have currently only implemented score for the hierarchical softmax scheme, so you need to have run word2vec with hs=1 and negative=0 for this to work."
     ]
    }
   ],
   "source": [
    "test_DF['likelihood'] = test_DF['example_token_clean'].apply(lambda x: adprob(x, book_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ad in test_DF.sort_values(by = 'likelihood', ascending = False)['example_text'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ad in test_DF.sort_values(by = 'likelihood')['example_text'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic chanage as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ascoDF = pandas.read_csv(\"../data/ASCO_abstracts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for wor2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascoDF['tokenized_sents'] = ascoDF['Body'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = lucem_illud.stop_words_basic) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating many embeddings so we have created this function to do most of the work. It creates two collections of embeddings, one the original and one the aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a couple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetWord = 'breast'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetWord = 'triple'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask which words changed the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most divergent words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordDivergences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordDivergences[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1b*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 3a or 3b.** Construct cells immediately below this that align word embeddings over time. Interrogate the spaces that result and ask which words change most of the whole period. What does this reveal about the social game underlying your space?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
