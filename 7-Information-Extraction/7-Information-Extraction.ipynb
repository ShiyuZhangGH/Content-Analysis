{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "Downloading ner from https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
      "Downloaded ner, extracting to ../stanford-NLP/ner\n",
      "Downloading parser from https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
      "Downloaded parser, extracting to ../stanford-NLP/parser\n",
      "Downloading core from http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
      "Downloaded core, extracting to ../stanford-NLP/core\n",
      "Downloading postagger from https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
      "Downloaded postagger, extracting to ../stanford-NLP/postagger\n",
      "Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have stanford-NLP setup before importing, so we are doing the import here. IF you have stanford-NLP working, you can import at the beginning like you would with any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we have defining our own `lucem_illud` function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `stanford.openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame. You can do this through links to the Stanford Core NLP project that we provide here, or play with their interface directly (in the penultimate cell of this notebook), which produces data in \"pretty graphics\" like this example parsing of the first sentence in the \"Shooting of Trayvon Martin\" Wikipedia article:\n",
    "\n",
    "![Output 1](../data/stanford_core1.png)\n",
    "![Output 2](../data/stanford_core2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', \n",
    "        'The quick brown fox jumped over the lazy dog.', \n",
    "        'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', \n",
    "        'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', \n",
    "        'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = stanford.postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  over_18  score                subreddit  \\\n",
       "4        goldie-gold    False  12650  Tales From Tech Support   \n",
       "3     TheDroolinFool    False  13152  Tales From Tech Support   \n",
       "2  Clickity_clickity    False  13404  Tales From Tech Support   \n",
       "1             SECGaz    False  13724  Tales From Tech Support   \n",
       "0   guitarsdontdance    False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " (')', 14),\n",
       " ('time', 14),\n",
       " ('computer', 12),\n",
       " ('lot', 12),\n",
       " ('email', 11),\n",
       " ('life', 11),\n",
       " ('**Genius**', 10),\n",
       " ('message', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('day', 9),\n",
       " ('story', 8),\n",
       " ('laptop', 8),\n",
       " ('call', 8),\n",
       " ('today', 8),\n",
       " ('part', 8),\n",
       " ('office', 8),\n",
       " ('problem', 7)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('help', 6),\n",
       " ('look', 6),\n",
       " ('send', 6),\n",
       " ('tell', 6),\n",
       " ('go', 5),\n",
       " ('feel', 4),\n",
       " ('take', 4),\n",
       " ('call', 4),\n",
       " ('receive', 4),\n",
       " ('thank', 4),\n",
       " ('work', 4),\n",
       " ('see', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'unrestricted'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tagger\n",
    "\n",
    "We can check the POS tagger by running it on a manually tagged corpus and identifying a reasonable error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Chinese tree bank\n",
    "ctb= nltk.corpus.sinica_treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The Precision is 96.547%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is quite good. Nevertheless, for a 20 word sentence, we only have a 66% chance ($1-.96^{20}$) of tagging (and later parsing) it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> I gathered some random text for this exercise. I watched some HPV commercials on youtube and I found that have very different advertising styles. I transcribed the 9 short videos. \n",
    "\n",
    "<span style=\"color:green\"> I watched these videos and I know the differences between them. This is a toy dataset for practicsing the codes. I do not get much new information from identifying noun and verb. \n",
    "\n",
    "<span style=\"color:green\"> Even though the vaccine is for both boys and girls, the commercial still have a focus on girls. The commercials also speak to mothers more. No adjective is included in these commercial. The commercial probably try to keep themselve more factual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Each year in the US, thousands of women learn ...</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I chose to get vaccinated because I will do ev...</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have cervical cancer from an infection of hu...</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I had taken action to prevent HPV, my daugh...</td>\n",
       "      <td>hk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In Hong Kong, more than 400 women are diagnose...</td>\n",
       "      <td>hk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am Sarah; I am sixteen. My name is Robin; I ...</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>She won’t always be this carefree. But your ch...</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I can’t protect myself against cervical cancer...</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Every day about thirty women in the US learned...</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ad      loc\n",
       "0  Each year in the US, thousands of women learn ...       us\n",
       "1  I chose to get vaccinated because I will do ev...       us\n",
       "2  I have cervical cancer from an infection of hu...       us\n",
       "3  If I had taken action to prevent HPV, my daugh...       hk\n",
       "4  In Hong Kong, more than 400 women are diagnose...       hk\n",
       "5  I am Sarah; I am sixteen. My name is Robin; I ...  ireland\n",
       "6  She won’t always be this carefree. But your ch...       us\n",
       "7  I can’t protect myself against cervical cancer...       uk\n",
       "8  Every day about thirty women in the US learned...       us"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpv0= \"Each year in the US, thousands of women learn they have cervical cancer. I could be one less. One less statistic. One less because now there’s Gardasil, the only vaccine that may help protect you from four types of human papillomavirus that may cause seventy percent of cervical cancer. I want to be one less women who battle cervical cancer. One less. Like all vaccines, Gardasil may not fully protect everyone. The side effects include pain, swelling, itching and redness at the injection site, fever, nausea, or dizziness. Gardasil is not for women who are pregnant. Gardasil does not prevent all types of cervical cancer. So it is important to continue with routine cervical cancer screening. Gardasil will not treat cervical cancer. Ask your doctor about getting back saved with the only cervical cancer vaccine. Gardasil, Gardasil, Gardasil. With Gardasil you could be one less. I want to be one less, one less.\"\n",
    "hpv1= \"I chose to get vaccinated because I will do everything I can to help protect myself from cervical cancer. I chose to get vaccinated when my doctor told me HPV can affect women my age and how Gardasil can help protect me. Gardasil is the only cervical cancer vaccine that helps protect against four types of HPV. Two types that cause seventy percent of cervical cancer and two more types that causes other HPV diseases. I chose to get vaccinated after my doctor told me cervical cancer isn’t the only HPV disease Gardasil helps prevent. Gardasil does not treat cervical cancer or other HPV diseases. Side effects include pain, swelling, itching and redness at the injection site, fever, nausea, dizziness, vomiting and fainting. Gardasil is not for women who are pregnant. Gardasil may not fully protect everyone and does not prevent all kinds of cervical cancer so it is important to continue routine cervical cancer screenings. I chose to get vaccinated because my dreams don’t include cervical cancer. Gardasil, Gardasil, Gardasil. You have the power to choose. Ask your doctor about Gardasil.\"\n",
    "hpv2= \"I have cervical cancer from an infection of human papillomavirus. Who knew HPV could lead to certain cancers? Who knew my risk for HPV would increase as I got older? Who knew that there is something that could have helped protect me from HPV when I was 11 or 12, way before I would even be exposed to it. Did you know, mom, dad?  Who knew HPV could cause certain cancers and diseases in girls? And boys. I was infected with HPV. maybe my parents didn’t know how widespread HPV is while HPV clears up for most that wasn’t the case for me. Maybe they didn’t know I would end up with cancer because of HPV. Maybe if they had known there was a vaccine to help protect me when I was 11 or 12 maybe my parents just didn't know. Right, mom, dad? What will you say? Don't wait talk to child’s doctor today. Learn more at HPV.com.\"\n",
    "hpv3= \"If I had taken action to prevent HPV, my daughter wouldn’t be infected by the virus. HPV can infect baby via the birth canal of mother. Winnie is now admitted to hospital for her fourth laryngeal surgery. Mom, I want to go home now. If you don’t want your loved ones to be affected, take action to prevent HPV virus now.\"\n",
    "hpv4= \"In Hong Kong, more than 400 women are diagnosed with cervical cancer every year in average. Nine types of HPV can cause ninety percent of cervical cancers. Do not hesitate, take action to prevent HPV virus now.\"\n",
    "hpv5= \"I am Sarah; I am sixteen. My name is Robin; I am fourteen. I am Song; I am twenty. My name is Lucy; I am eighteen. Cervical cancer affects the neck of the women. It is the second most common cancer between the age of twenty-five and thirty-nine. My name is Rosina; I am twenty-five years old. My name is Ketie; I am nineteen. I am Lowise; I am twenty-one. I am Dara; I am fifteen years old. Three hundred women will develop cervical cancer, and ninety women unfortunately will die from cervical cancer every year. My mom has got cervical cancer so I am at a really high risk. All mothers want what’s best for their children and HPV vaccination is one way of preventing cervical cancer. I got the information from Alicia at my school. You learned what the HPV vaccine was what do we feel like to get it and that it lowers the risk of cervical cancer. There is a wealthy of reputable information on the HSE website that can help you and your family make an informed decision regarding HPV vaccination. I got the HPV vaccine, I got the HPV vaccine; I got the HPV vaccine; I got the HPV vaccine; I got the HPV vaccine. There is excellent scientific evidence that shows that HPV vaccination prevents cervical cancer. Get the vaccine. Get the vaccine. Get the vaccine. Protect our future.\"\n",
    "hpv6= \"She won’t always be this carefree. But your choices now can protect her from cancer later. The HPV vaccine can protect her from certain cancers and other diseases caused by HPV. Girls and boys should be vaccinated at eleven or twelve or as soon as possible if they are older. Get the vaccine today and worry less about cancer tomorrow. HPV vaccine is cancer prevention. Talk to your child’s doctor or visit health.ny.gov/HPV.\"\n",
    "hpv7= \"I can’t protect myself against cervical cancer. Now every year nine girl can thanks to the new HPV vaccine. It can prevent hundreds of deaths from cervical cancer in the UK every year. The HPV vaccine is now being offered in states to all year nine girls. So if you are a parent or a girl in year nine and want more information then visit www.helpprotectyourself.info.\"\n",
    "hpv8= \"Every day about thirty women in the US learned that they have cervical cancer. That’s why I chose to get my daughter vaccinated. I chose to get my daughter vaccinated when her doctor and I agreed that the right time to protect her is now because it’s about prevention. Gardasil is the only cervical cancer vaccine to protect against four types of HPV. Two types cause seventy percent of cervical cancer and two more types that causes other HPV diseases. I chose to get my daughter vaccinated because the CDC recommends that girls her age get vaccinated. Gardasil does not treat cervical cancer or other HPV diseases. Gardasil may not fully protect everyone and does not prevent all kinds of cervical cancer. So it is important to continue routine cervical cancer screening. I chose to get my daughter vaccinated because I want her to be one less women affected by cervical cancer. One less. Guard herself. Ask your daughter’s daughter about Gardasil.\"\n",
    "\n",
    "\n",
    "hpvDF= pandas.DataFrame({\"ad\": [hpv0, hpv1, hpv2, hpv3, hpv4, hpv5, hpv6, hpv7, hpv8], \n",
    "                         \"loc\": [\"us\", \"us\", \"us\", \"hk\", \"hk\", \"ireland\", \"us\", \"uk\", \"us\"]})\n",
    "hpvDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hpvDF['sentences'] = hpvDF['ad'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "hpvDF['POS_sents'] = hpvDF['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(Each, DT), (year, NN), (in, IN), (the, DT),...\n",
       "1    [[(I, PRP), (chose, VBD), (to, TO), (get, VB),...\n",
       "2    [[(I, PRP), (have, VBP), (cervical, JJ), (canc...\n",
       "3    [[(If, IN), (I, PRP), (had, VBD), (taken, VBN)...\n",
       "4    [[(In, IN), (Hong, NNP), (Kong, NNP), (,, ,), ...\n",
       "5    [[(I, PRP), (am, VBP), (Sarah, NNP), (;, :), (...\n",
       "6    [[(She, PRP), (won, VBD), (’, CD), (t, NN), (a...\n",
       "7    [[(I, PRP), (can, MD), (’, VB), (t, NN), (prot...\n",
       "8    [[(Every, DT), (day, NN), (about, IN), (thirty...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpvDF['POS_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cancer', 38), ('HPV', 22), ('vaccine', 19), ('t', 9), ('daughter', 7), ('year', 7), ('doctor', 7), ('s', 5), ('name', 4), ('percent', 4), ('age', 3), ('mom', 3), ('everyone', 3), ('virus', 3), ('action', 3), ('risk', 3), ('vaccination', 3), ('information', 3), ('dizziness', 2), ('itching', 2)]\n"
     ]
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in hpvDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "print(sortedTargets[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('protect', 14), ('be', 10), ('get', 10), ('prevent', 8), ('help', 5), ('Get', 4), ('treat', 3), ('cause', 3), ('Ask', 3), ('continue', 3), ('know', 2), ('want', 2), ('take', 2), ('have', 1), ('Protect', 1), ('wait', 1), ('Learn', 1), ('increase', 1), ('worry', 1), ('feel', 1)]\n"
     ]
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in hpvDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "print(sortedTargets[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in hpvDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = stanford.nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, O), (year, O), (,, O), (Help, O), (De...\n",
       "8    [[(First, O), (post, O), (in, O), (quite, O), ...\n",
       "7    [[([, O), (Original, O), (Post, O), (], O), ((...\n",
       "6    [[(I, O), (witnessed, O), (this, O), (astoundi...\n",
       "5    [[(I, O), (work, O), (Helpdesk, ORGANIZATION),...\n",
       "4    [[(This, O), (just, O), (happened, O), (..., O...\n",
       "3    [[(Another, O), (tale, O), (from, O), (the, O)...\n",
       "2    [[([, O), (Part, O), (1, O), (], O), ((, O), (...\n",
       "1    [[(>, O), ($, O), (Me, O), (-, O), (Hello, O),...\n",
       "0    [[(So, O), (my, O), (story, O), (starts, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 401),\n",
       " ('I', 245),\n",
       " ('the', 226),\n",
       " (',', 205),\n",
       " ('to', 197),\n",
       " ('a', 143),\n",
       " ('and', 135),\n",
       " ('>', 106),\n",
       " ('you', 102),\n",
       " ('of', 97)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understand',\n",
       " 'BING',\n",
       " 'working',\n",
       " 'seconds',\n",
       " 'live',\n",
       " 'um',\n",
       " 'first',\n",
       " 'business',\n",
       " 'bitter',\n",
       " 'blinked',\n",
       " 'whose',\n",
       " 'error',\n",
       " 'turn',\n",
       " '=',\n",
       " 'holiday',\n",
       " '*I',\n",
       " 'took',\n",
       " 'certain',\n",
       " 'course',\n",
       " 'supervisor',\n",
       " \"'P4ssword\",\n",
       " 'existence',\n",
       " 'ok',\n",
       " 'ask',\n",
       " 'party',\n",
       " 'looking',\n",
       " 'loved',\n",
       " 'Everything',\n",
       " 'risks',\n",
       " 'pay',\n",
       " 'gildings',\n",
       " 'generate',\n",
       " 'drawer*',\n",
       " 'academic',\n",
       " 'soon',\n",
       " 'pages',\n",
       " 'Steve',\n",
       " 'avoid',\n",
       " 'plugged',\n",
       " 'occasionally',\n",
       " 'well',\n",
       " 'person',\n",
       " 'reply',\n",
       " 'heard',\n",
       " 'using',\n",
       " '*Are',\n",
       " 'played',\n",
       " 'needed',\n",
       " 'spent',\n",
       " 'food',\n",
       " 'mind',\n",
       " 'shaking',\n",
       " 'completely',\n",
       " 'year',\n",
       " 'case',\n",
       " 'done',\n",
       " 'login',\n",
       " 'often',\n",
       " 'stuff',\n",
       " 'stronger',\n",
       " 'tears',\n",
       " 'proud',\n",
       " 'organization',\n",
       " 'wrong',\n",
       " 'avalanche',\n",
       " 'In',\n",
       " 'revealing',\n",
       " 'speak',\n",
       " 'moment',\n",
       " 'passed',\n",
       " 'forwarded',\n",
       " 'lunch',\n",
       " 'watched',\n",
       " 'brought',\n",
       " 'mother',\n",
       " 'bane',\n",
       " 'Give',\n",
       " 'believe',\n",
       " 'search',\n",
       " 'cry',\n",
       " '#',\n",
       " 'stopped',\n",
       " 'Then',\n",
       " 'asset',\n",
       " 'Things',\n",
       " 'mess',\n",
       " 'bad',\n",
       " 'store',\n",
       " 'same',\n",
       " 'month',\n",
       " 'happiness',\n",
       " 'SO',\n",
       " 'window',\n",
       " 'four',\n",
       " 'Windows',\n",
       " 'discover',\n",
       " 'issues',\n",
       " 'door',\n",
       " 'since',\n",
       " 'yourself',\n",
       " 'ready',\n",
       " 'systems',\n",
       " 'favor',\n",
       " 'nothing',\n",
       " 'Whatever',\n",
       " 'yes',\n",
       " 'personally',\n",
       " 'share',\n",
       " 'staff',\n",
       " 'nice',\n",
       " 'local',\n",
       " 'P',\n",
       " 'retail',\n",
       " 'box',\n",
       " 'me*',\n",
       " 'Thursday',\n",
       " 'earlier',\n",
       " 'fix',\n",
       " 'guide',\n",
       " 'Original',\n",
       " 'computering',\n",
       " 'Wow',\n",
       " 'enjoy',\n",
       " 'Here',\n",
       " 'properly',\n",
       " 'above',\n",
       " 'okay',\n",
       " 'ran',\n",
       " 'While',\n",
       " 'whom',\n",
       " 'Computer',\n",
       " 'nasty',\n",
       " 'guy',\n",
       " 'building',\n",
       " 'One',\n",
       " 'upside',\n",
       " 'operate',\n",
       " 'helped',\n",
       " 'used',\n",
       " 'pointed',\n",
       " '*type',\n",
       " 'S',\n",
       " 'stupid',\n",
       " '5',\n",
       " 'myself',\n",
       " 'expire',\n",
       " 'check',\n",
       " 'making',\n",
       " 'anyway',\n",
       " '*Note',\n",
       " 'name',\n",
       " 'DVD',\n",
       " 'couple',\n",
       " 'terrible',\n",
       " 'brave',\n",
       " 'meant',\n",
       " 'busy',\n",
       " 'insurance',\n",
       " 'Turns',\n",
       " 'XYZ',\n",
       " 'real',\n",
       " 'opened',\n",
       " 'ago',\n",
       " 'immediately',\n",
       " 'THIS',\n",
       " '100',\n",
       " 'order',\n",
       " 'yelled',\n",
       " 'taking',\n",
       " 'sharing',\n",
       " 'future',\n",
       " 'mouse',\n",
       " 'arms',\n",
       " 'THE',\n",
       " 'weeks',\n",
       " 'There',\n",
       " 'comments',\n",
       " 'received',\n",
       " 'slightly',\n",
       " 'way',\n",
       " 'week',\n",
       " 'learn',\n",
       " 'try',\n",
       " 'Sure',\n",
       " 'Desk',\n",
       " 'suggested',\n",
       " '17',\n",
       " 'random',\n",
       " 'times',\n",
       " 'response',\n",
       " 'hit',\n",
       " 'service',\n",
       " 'generic',\n",
       " 'echo',\n",
       " 'large',\n",
       " 'absolutely',\n",
       " 'potential',\n",
       " 'themselves',\n",
       " 'its',\n",
       " 'cancer',\n",
       " 'step',\n",
       " 'family',\n",
       " 'died',\n",
       " 'older',\n",
       " 'videos',\n",
       " 'select',\n",
       " 'small',\n",
       " 'stand',\n",
       " 'ALL',\n",
       " 'gods',\n",
       " 'point',\n",
       " 'LOWERCASE',\n",
       " '60',\n",
       " 'return',\n",
       " 'web',\n",
       " 'DeskMugPhonePencil1',\n",
       " 'thinking',\n",
       " 'Thanks',\n",
       " 'however',\n",
       " 'pretty',\n",
       " 'Fail',\n",
       " 'scenario',\n",
       " 'Everyone',\n",
       " 'connection',\n",
       " 'difference',\n",
       " 'drowned',\n",
       " 'calls',\n",
       " 'meet',\n",
       " 'living',\n",
       " 'command',\n",
       " 'idea',\n",
       " 'anymore',\n",
       " 'nose',\n",
       " 'Of',\n",
       " 'sound',\n",
       " 'both',\n",
       " 'shortcut',\n",
       " 'allowed',\n",
       " 'mistakes',\n",
       " 'three',\n",
       " 'hear',\n",
       " 'site',\n",
       " 'others',\n",
       " 'different',\n",
       " 'key',\n",
       " 'information',\n",
       " 'closed',\n",
       " 'mailboxes',\n",
       " 'types',\n",
       " 'lived',\n",
       " 'flaws',\n",
       " 'situation',\n",
       " 'sometimes',\n",
       " 'glad',\n",
       " 'User',\n",
       " 'Ca',\n",
       " 'CEO',\n",
       " 'willing',\n",
       " 'visit',\n",
       " 'request',\n",
       " 'HR']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 17),\n",
       " ('Google', 6),\n",
       " ('Smith', 5),\n",
       " ('Steve', 2),\n",
       " ('Boss', 1),\n",
       " ('CMD', 1),\n",
       " ('Buzzfeed', 1),\n",
       " ('Spotify', 1),\n",
       " ('Nono', 1),\n",
       " ('Clickity', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 6), ('CMD', 1), ('GOOGLE', 1), ('Helpdesk', 1), ('Citrix', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> The commercials target on general public and assumes no backgound information. The code recongize \"Gardasil\" as an organization. But it is actually a vaccine for HPV. Several human names were identified as non-objects. A couple of videos appear to emotions and invite girls to speak in front of the camera \"I am xxx\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hpvDF['classified_sents'] = hpvDF['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 93),\n",
       " ('I', 44),\n",
       " ('cancer', 38),\n",
       " ('HPV', 35),\n",
       " ('the', 34),\n",
       " ('cervical', 33),\n",
       " ('to', 32),\n",
       " (',', 31),\n",
       " ('Gardasil', 23),\n",
       " ('of', 21)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in hpvDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gardasil', 23),\n",
       " ('US', 2),\n",
       " ('Lowise', 1),\n",
       " ('Lucy', 1),\n",
       " ('Robin', 1),\n",
       " ('Rosina', 1),\n",
       " ('Sarah', 1),\n",
       " ('Ketie', 1),\n",
       " ('UK', 1),\n",
       " ('Winnie', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in hpvDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. The parser is a dependency parser, but this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in is the relationship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice in two different nested noun phrases in the sentence. \n",
    "\n",
    "We can also find all of the verbs within the noun phrase defined by one or more target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shot'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(fourthSentParseTree, 'NP', 'VBN', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ROOT                           \n",
      "                      |                              \n",
      "                      S                             \n",
      "       _______________|___________________________   \n",
      "      |                          VP               | \n",
      "      |                __________|___             |  \n",
      "      |               |              PP           | \n",
      "      |               |      ________|___         |  \n",
      "      NP              |     |            NP       | \n",
      "  ____|__________     |     |     _______|____    |  \n",
      " DT   JJ    JJ   NN  VBD    IN   DT      JJ   NN  . \n",
      " |    |     |    |    |     |    |       |    |   |  \n",
      "The quick brown fox jumped over the     lazy dog  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x1196492f0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'The'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'quick'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'brown'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [2, 3],\n",
      "                                      'det': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'fox'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'nmod': [9],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'jumped'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'over'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'lazy'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [8],\n",
      "                                      'case': [6],\n",
      "                                      'det': [7]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'dog'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another. We note that unless you have the graphviz successfully installed on your computer (which is not necessary to complete this homework), the following graphviz call will trigger an error. If you are interested in installing graphviz and working on a Mac, consider installing through [homebrew](https://brew.sh), a package manager (i.e., with the command \"brew install graphviz\", once brew is installed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"467pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 467.37 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 463.3657,-298 463.3657,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (jumped)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.7949,-257.7616C193.7949,-246.3597 193.7949,-231.4342 193.7949,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"197.295,-218.2121 193.7949,-208.2121 190.295,-218.2121 197.295,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"205.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"152.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.9506,-171.6975C182.2192,-166.0286 179.2067,-159.7594 176.457,-154 172.9867,-146.731 169.2503,-138.8578 165.7948,-131.5568\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.7462,-129.6107 161.3081,-122.0658 162.4177,-132.6024 168.7462,-129.6107\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"316.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (dog)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.5798,-171.9716C237.9481,-159.1286 262.8214,-141.7376 282.8098,-127.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.8541,-130.6033 291.044,-122.0047 280.843,-124.8665 284.8541,-130.6033\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.7397\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"108.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (quick)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"195.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (brown)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126.8005,-85.9716C108.2827,-73.1286 83.2073,-55.7376 63.0563,-41.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.967,-38.8277 54.7552,-36.0047 60.9777,-44.5797 64.967,-38.8277\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4637,-85.7616C137.4551,-74.0176 129.534,-58.5355 122.7802,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7834,-43.5204 118.1127,-36.2121 119.5517,-46.7088 125.7834,-43.5204\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.9141,-85.7616C167.7861,-74.0176 175.5272,-58.5355 182.1275,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.3472,-46.7216 186.6889,-36.2121 179.0862,-43.5911 185.3472,-46.7216\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"279.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>9&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.9482,-85.7616C303.9446,-74.1316 297.3638,-58.8357 291.721,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.7976,-44.0148 287.6304,-36.2121 288.3674,-46.7813 294.7976,-44.0148\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.8398\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"354.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M324.8537,-85.7616C329.9926,-74.1316 336.7512,-58.8357 342.5466,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"345.9074,-46.7736 346.7477,-36.2121 339.5046,-43.9444 345.9074,-46.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"429.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (lazy)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M340.4834,-85.9716C357.2078,-73.2433 379.8019,-56.0478 398.0799,-42.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.2997,-44.8461 406.1376,-36.0047 396.0604,-39.2758 400.2997,-44.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"396.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11978fe10>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"902pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 901.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 897.9575,-556 897.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"232.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"232.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M232.3833,-515.7616C232.3833,-504.3597 232.3833,-489.4342 232.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.8834,-476.2121 232.3833,-466.2121 228.8834,-476.2121 235.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"74.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M192.5259,-429.8504C180.9641,-424.3453 168.399,-418.1273 157.0454,-412 141.9107,-403.8322 125.6384,-394.1858 111.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.0198,-382.3384 102.6735,-380.0569 109.3388,-388.2925 113.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"158.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M216.6898,-429.7616C206.1921,-417.5615 192.2231,-401.3273 180.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"183.2297,-385.5094 174.0542,-380.2121 177.9236,-390.0751 183.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"232.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M232.3833,-429.7616C232.3833,-418.3597 232.3833,-403.4342 232.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.8834,-390.2121 232.3833,-380.2121 228.8834,-390.2121 235.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"316.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M250.1975,-429.7616C262.2253,-417.4475 278.2673,-401.0235 291.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.1111,-389.8115 298.5947,-380.2121 289.1035,-384.9203 294.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"309.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"418.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M278.035,-436.267C298.523,-430.2297 322.6866,-422.0184 343.3833,-412 358.1468,-404.8536 373.3934,-395.1776 386.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"388.6132,-388.8578 394.7385,-380.213 384.5592,-383.1511 388.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"383.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M67.3848,-343.7616C62.9222,-332.1316 57.0528,-316.8357 52.02,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.2219,-302.2945 48.3717,-294.2121 48.6866,-304.8023 55.2219,-302.2945\"/>\n",
       "<text text-anchor=\"middle\" x=\"89.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M104.1951,-343.8208C111.0895,-338.6633 117.9793,-332.6262 123.3833,-326 128.7334,-319.4399 133.135,-311.4322 136.6039,-303.7643\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"139.9028,-304.9461 140.5002,-294.3681 133.4367,-302.2648 139.9028,-304.9461\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"280.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M373.3287,-344.0907C361.8967,-338.8521 349.8661,-332.7092 339.2935,-326 327.8536,-318.7405 316.2709,-309.4264 306.4387,-300.809\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.7445,-298.1755 298.9649,-294.0999 304.0683,-303.3846 308.7445,-298.1755\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"366.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M386.9119,-343.7718C380.7813,-338.7844 375.0843,-332.8348 371.2798,-326 367.6468,-319.4733 365.8966,-311.7178 365.176,-304.286\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.6673,-304.0028 364.7539,-294.1572 361.6734,-304.2943 368.6673,-304.0028\"/>\n",
       "<text text-anchor=\"middle\" x=\"400.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"454.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M426.018,-343.7616C430.8863,-332.1316 437.2893,-316.8357 442.7796,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.1267,-304.788 446.7596,-294.2121 439.6696,-302.085 446.1267,-304.788\"/>\n",
       "<text text-anchor=\"middle\" x=\"461.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"547.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M457.4282,-343.9087C467.763,-338.5897 478.7248,-332.4531 488.3833,-326 499.4601,-318.5993 510.8133,-309.4245 520.5648,-300.9626\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"523.1734,-303.3276 528.3381,-294.0768 518.5319,-298.0877 523.1734,-303.3276\"/>\n",
       "<text text-anchor=\"middle\" x=\"523.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"246.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M421.1702,-260.9437C418.2294,-259.8618 415.2725,-258.8603 412.3833,-258 368.0759,-244.8067 352.1421,-258.8621 309.9351,-240 296.1147,-233.8237 282.6618,-223.9798 271.7188,-214.6536\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.0307,-212.0258 264.2247,-208.014 269.3886,-217.2652 274.0307,-212.0258\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"328.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M421.1784,-260.4383C409.7229,-254.5703 397.0234,-247.4896 386.0659,-240 375.2101,-232.5799 364.1045,-223.4008 354.5734,-214.9408\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"356.738,-212.1792 346.9776,-208.058 352.0377,-217.3664 356.738,-212.1792\"/>\n",
       "<text text-anchor=\"middle\" x=\"401.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"409.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M438.0881,-257.7967C433.7634,-252.3398 429.379,-246.1607 426.0591,-240 422.3584,-233.1328 419.2732,-225.2945 416.8039,-217.8935\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"420.108,-216.7304 413.8185,-208.2044 413.4184,-218.7917 420.108,-216.7304\"/>\n",
       "<text text-anchor=\"middle\" x=\"448.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"499.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M463.9266,-257.7616C470.0718,-246.0176 478.1729,-230.5355 485.0802,-217.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"488.3186,-218.6951 489.8537,-208.2121 482.1163,-215.4497 488.3186,-218.6951\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"612.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M487.5792,-260.8936C500.7359,-254.6924 515.9102,-247.2707 529.3833,-240 544.7081,-231.7301 561.2104,-221.973 575.4323,-213.2835\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"577.2801,-216.2561 583.9618,-208.0339 573.611,-210.2947 577.2801,-216.2561\"/>\n",
       "<text text-anchor=\"middle\" x=\"569.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"761.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M487.6128,-260.9978C490.55,-259.9042 493.5016,-258.8852 496.3833,-258 536.6277,-245.6374 548.4602,-249.8864 589.3833,-240 630.2385,-230.13 675.8976,-216.7448 709.9,-206.3074\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"711.0902,-209.603 719.6127,-203.3088 709.0252,-202.9145 711.0902,-209.603\"/>\n",
       "<text text-anchor=\"middle\" x=\"658.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"296.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"377.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"377.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M377.3833,-85.7616C377.3833,-74.3597 377.3833,-59.4342 377.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.8834,-46.2121 377.3833,-36.2121 373.8834,-46.2121 380.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"402.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M321.5969,-171.7616C317.2695,-160.1316 311.578,-144.8357 306.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"309.9276,-130.3637 303.1599,-122.2121 303.367,-132.8049 309.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"326.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M338.7749,-171.7616C345.5313,-159.9036 354.459,-144.2345 362.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"365.0971,-132.6334 367.0066,-122.2121 359.0151,-129.1681 365.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"393.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M573.1869,-171.9496C562.328,-166.5419 550.6763,-160.3501 540.2935,-154 527.6297,-146.2549 514.3446,-136.8017 502.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"505.0333,-125.4208 494.9593,-122.1398 500.7884,-130.9869 505.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"552.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"560.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M589.2392,-171.951C583.6851,-166.6739 578.2261,-160.5473 574.2798,-154 570.2509,-147.3157 567.3575,-139.4301 565.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"568.6687,-130.9902 562.9608,-122.0643 561.8567,-132.6015 568.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"603.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"668.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M624.2595,-171.7616C631.981,-159.9036 642.1841,-144.2345 650.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"654.0005,-132.502 656.5243,-122.2121 648.1345,-128.6822 654.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"660.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"761.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M761.3833,-171.7616C761.3833,-160.3597 761.3833,-145.4342 761.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"764.8834,-132.2121 761.3833,-122.2121 757.8834,-132.2121 764.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"773.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"850.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M780.2579,-171.7616C793.0016,-159.4475 809.9985,-143.0235 824.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"826.7767,-131.6779 831.5359,-122.2121 821.9125,-126.644 826.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"840.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"563.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M646.3719,-85.9716C630.9715,-73.358 610.2148,-56.3573 593.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"595.3198,-39.6334 585.3658,-36.0047 590.8843,-45.0488 595.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"630.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"668.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.3833,-85.7616C668.3833,-74.3597 668.3833,-59.4342 668.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"671.8834,-46.2121 668.3833,-36.2121 664.8834,-46.2121 671.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"697.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"784.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M703.3154,-85.9914C712.713,-80.629 722.6789,-74.4499 731.3833,-68 741.2881,-60.6606 751.3323,-51.6634 759.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"762.5027,-45.7565 767.1694,-36.2448 757.5896,-40.7703 762.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"778.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11978f5f8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topPostDepParse = list(stanford.depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1014pt\" height=\"818pt\"\n",
       " viewBox=\"0.00 0.00 1014.21 818.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 814)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-814 1010.207,-814 1010.207,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"207.2432\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"207.2432\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M207.2432,-773.7616C207.2432,-762.3597 207.2432,-747.4342 207.2432,-734.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.7433,-734.2121 207.2432,-724.2121 203.7433,-734.2121 210.7433,-734.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.519\" y=\"-744.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"37.2432\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M180.0991,-700.0143C157.3043,-694.3052 124.3518,-684.4249 98.1396,-670 85.8462,-663.2347 73.6272,-653.8632 63.3969,-645.0603\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"65.4559,-642.2079 55.6552,-638.1831 60.8069,-647.4412 65.4559,-642.2079\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.7949\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"122.2432\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M180.0131,-688.3776C172.5318,-682.9299 164.6844,-676.6103 158.1396,-670 151.1828,-662.9735 144.5653,-654.4941 138.9508,-646.5301\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"141.7966,-644.4907 133.2834,-638.1838 136.0055,-648.423 141.7966,-644.4907\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.7949\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"207.2432\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M207.2432,-687.7616C207.2432,-676.3597 207.2432,-661.4342 207.2432,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.7433,-648.2121 207.2432,-638.2121 203.7433,-648.2121 210.7433,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.4121\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"295.2432\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M225.9057,-687.7616C238.5062,-675.4475 255.3121,-659.0235 269.2225,-645.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"271.9019,-647.7046 276.6075,-638.2121 267.0094,-642.6983 271.9019,-647.7046\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.3501\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"450.2432\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M234.646,-696.3019C278.1919,-680.8906 363.2883,-650.7742 412.5475,-633.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"413.9335,-636.5631 422.1928,-629.9273 411.598,-629.9642 413.9335,-636.5631\"/>\n",
       "<text text-anchor=\"middle\" x=\"356.688\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"133.2432\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M261.2827,-601.9716C235.9692,-588.5336 201.276,-570.1162 174.3874,-555.842\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"175.704,-552.5783 165.2303,-550.9808 172.4217,-558.7612 175.704,-552.5783\"/>\n",
       "<text text-anchor=\"middle\" x=\"234.3501\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"252.2432\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M277.2835,-601.7041C272.7881,-596.3406 268.3405,-590.2327 265.1396,-584 261.688,-577.279 259.1113,-569.564 257.2075,-562.238\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5743,-561.2589 254.9431,-552.2844 253.7487,-562.8117 260.5743,-561.2589\"/>\n",
       "<text text-anchor=\"middle\" x=\"294.7949\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"357.2432\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M310.3378,-601.948C314.9187,-596.2894 319.8887,-589.9629 324.2432,-584 329.6217,-576.6347 335.1875,-568.4487 340.181,-560.8685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"343.2143,-562.6242 345.7348,-552.3336 337.3471,-558.8064 343.2143,-562.6242\"/>\n",
       "<text text-anchor=\"middle\" x=\"342.4019\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"450.2432\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.2432,-601.7616C450.2432,-590.3597 450.2432,-575.4342 450.2432,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"453.7433,-562.2121 450.2432,-552.2121 446.7433,-562.2121 453.7433,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"458.7949\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"550.2432\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M471.4506,-601.7616C485.9019,-589.3335 505.221,-572.719 521.1125,-559.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"523.7666,-561.3862 529.0663,-552.2121 519.2023,-556.0789 523.7666,-561.3862\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.188\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"473.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M533.9134,-515.7616C522.9901,-503.5615 508.4548,-487.3273 496.35,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"498.8274,-471.3276 489.5493,-466.2121 493.6123,-475.9969 498.8274,-471.3276\"/>\n",
       "<text text-anchor=\"middle\" x=\"529.2881\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"550.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M550.2432,-515.7616C550.2432,-504.3597 550.2432,-489.4342 550.2432,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"553.7433,-476.2121 550.2432,-466.2121 546.7433,-476.2121 553.7433,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"558.7949\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"631.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M567.4212,-515.7616C578.912,-503.5615 594.2023,-487.3273 606.936,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"609.7814,-475.8913 614.0899,-466.2121 604.6858,-471.0919 609.7814,-475.8913\"/>\n",
       "<text text-anchor=\"middle\" x=\"611.7949\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"721.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M593.0332,-515.8322C605.4626,-510.3267 618.987,-504.113 631.2432,-498 647.9569,-489.6637 666.0209,-479.8025 681.5483,-471.05\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"683.3984,-474.0245 690.3681,-466.0448 679.9435,-467.9365 683.3984,-474.0245\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.7813\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"563.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.884,-432.4737C671.4995,-426.4508 656.3352,-419.2608 642.9053,-412 628.0589,-403.9733 612.215,-394.2776 598.5992,-385.5689\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.305,-382.504 590.0064,-380.0161 596.5057,-388.3833 600.305,-382.504\"/>\n",
       "<text text-anchor=\"middle\" x=\"658.4121\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"641.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M704.2772,-429.7616C692.9283,-417.5615 677.8267,-401.3273 665.2502,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"667.5584,-385.1501 658.1847,-380.2121 662.4331,-389.9179 667.5584,-385.1501\"/>\n",
       "<text text-anchor=\"middle\" x=\"696.3501\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"721.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M721.2432,-429.7616C721.2432,-418.3597 721.2432,-403.4342 721.2432,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"724.7433,-390.2121 721.2432,-380.2121 717.7433,-390.2121 724.7433,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"743.7949\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"806.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M748.9176,-429.9916C756.2288,-424.6292 763.8548,-418.45 770.2432,-412 777.2081,-404.9678 783.8413,-396.4869 789.4727,-388.5232\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"792.4204,-390.4128 795.1585,-380.1779 786.6355,-386.4715 792.4204,-390.4128\"/>\n",
       "<text text-anchor=\"middle\" x=\"788.457\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"892.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M757.4665,-431.5318C770.4032,-425.5274 785.0405,-418.5922 798.2432,-412 815.3854,-403.4407 834.0569,-393.6154 850.2088,-384.9525\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"852.2519,-387.8276 859.397,-380.0047 848.9329,-381.6644 852.2519,-387.8276\"/>\n",
       "<text text-anchor=\"middle\" x=\"837.2949\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"854.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M873.8839,-343.8974C869.3869,-338.5346 865.0384,-332.3788 862.1396,-326 859.0753,-319.2569 857.1728,-311.5343 855.9993,-304.2081\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.4705,-303.7587 854.7805,-294.2585 852.5224,-304.6099 859.4705,-303.7587\"/>\n",
       "<text text-anchor=\"middle\" x=\"884.7949\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"937.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M901.7865,-343.7616C907.9316,-332.0176 916.0327,-316.5355 922.94,-303.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"926.1784,-304.6951 927.7136,-294.2121 919.9762,-301.4497 926.1784,-304.6951\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.188\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"899.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M929.1843,-257.7616C924.0455,-246.1316 917.2869,-230.8357 911.4915,-217.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"914.5335,-215.9444 907.2904,-208.2121 908.1307,-218.7736 914.5335,-215.9444\"/>\n",
       "<text text-anchor=\"middle\" x=\"933.2881\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"976.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M945.5141,-257.7616C950.8398,-246.0176 957.8608,-230.5355 963.8471,-217.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"967.0416,-218.7649 967.9842,-208.2121 960.6665,-215.8739 967.0416,-218.7649\"/>\n",
       "<text text-anchor=\"middle\" x=\"966.7949\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"34.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"114.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M112.2478,-515.7616C97.941,-503.3335 78.8151,-486.719 63.0825,-473.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"65.0529,-470.1279 55.2083,-466.2121 60.4623,-475.4124 65.0529,-470.1279\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.3501\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M129.2138,-515.7616C126.6695,-504.2456 123.3312,-489.1353 120.4527,-476.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.8417,-475.2216 118.2668,-466.2121 117.0065,-476.7317 123.8417,-475.2216\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.3501\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"197.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.8159,-515.7616C155.7253,-503.7896 167.5257,-487.9328 177.4712,-474.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"180.5277,-476.324 183.69,-466.2121 174.912,-472.1449 180.5277,-476.324\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.9019\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"54.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M160.7847,-429.8674C150.2297,-424.3627 138.7795,-418.1406 128.4775,-412 114.9215,-403.9198 100.4376,-394.3813 87.8957,-385.8132\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"89.7783,-382.86 79.5583,-380.0664 85.8056,-388.6235 89.7783,-382.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.626\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"131.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M183.2463,-429.7616C173.9709,-417.6756 161.6572,-401.6304 151.3405,-388.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"154.0847,-386.0143 145.2199,-380.2121 148.5315,-390.2761 154.0847,-386.0143\"/>\n",
       "<text text-anchor=\"middle\" x=\"183.4121\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"218.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M201.6967,-429.7616C204.5088,-418.2456 208.1985,-403.1353 211.38,-390.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.8239,-390.757 213.796,-380.2121 208.0237,-389.0964 214.8239,-390.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.188\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"148.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M203.398,-343.7616C193.5605,-331.6756 180.5005,-315.6304 169.5585,-302.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.0942,-299.7583 163.067,-294.2121 166.6652,-304.1772 172.0942,-299.7583\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.2881\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"225.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.7277,-343.7616C220.6557,-332.3597 221.8706,-317.4342 222.9239,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"226.4379,-304.4631 223.7608,-294.2121 219.461,-303.8952 226.4379,-304.4631\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.5811\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"292.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M343.4583,-515.7616C334.4097,-503.7896 322.4249,-487.9328 312.324,-474.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"314.83,-472.0794 306.0081,-466.2121 309.2456,-476.3002 314.83,-472.0794\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.626\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"380.2432\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M362.1209,-515.7616C365.2007,-504.2456 369.2419,-489.1353 372.7263,-476.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"376.17,-476.7769 375.3725,-466.2121 369.4076,-474.9683 376.17,-476.7769\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.688\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"308.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M364.9738,-429.7616C354.8553,-417.6756 341.4221,-401.6304 330.1675,-388.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.5936,-385.6329 323.4905,-380.2121 327.2262,-390.1265 332.5936,-385.6329\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.7949\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"392.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M382.7881,-429.7616C384.379,-418.3597 386.4617,-403.4342 388.2673,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.7863,-390.5999 389.7019,-380.2121 384.8535,-389.6324 391.7863,-390.5999\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.2949\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"475.2432\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M400.3902,-429.7616C414.119,-417.3335 432.4721,-400.719 447.569,-387.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"450.0605,-389.518 455.1251,-380.2121 445.3627,-384.3286 450.0605,-389.518\"/>\n",
       "<text text-anchor=\"middle\" x=\"446.2949\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"332.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M379.5187,-343.7616C371.1661,-331.7896 360.1033,-315.9328 350.7793,-302.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"353.5416,-300.4107 344.9493,-294.2121 347.8007,-304.416 353.5416,-300.4107\"/>\n",
       "<text text-anchor=\"middle\" x=\"381.4019\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"461.2432\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M406.8763,-343.7616C416.5732,-331.6756 429.4467,-315.6304 440.2324,-302.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.103,-304.2023 446.6311,-294.2121 437.6431,-299.8216 443.103,-304.2023\"/>\n",
       "<text text-anchor=\"middle\" x=\"449.9019\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"238.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M312.3082,-257.7616C298.724,-245.3335 280.564,-228.719 265.626,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"267.89,-212.3799 258.1494,-208.2121 263.1649,-217.5446 267.89,-212.3799\"/>\n",
       "<text text-anchor=\"middle\" x=\"305.626\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"332.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.2432,-257.7616C332.2432,-246.3597 332.2432,-231.4342 332.2432,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.7433,-218.2121 332.2432,-208.2121 328.7433,-218.2121 335.7433,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"344.688\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"418.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M452.124,-257.7616C446.252,-246.0176 438.5109,-230.5355 431.9106,-217.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"434.9519,-215.5911 427.3492,-208.2121 428.6909,-218.7216 434.9519,-215.5911\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.626\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"496.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M468.6658,-257.7616C473.3989,-246.1316 479.624,-230.8357 484.9618,-217.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"488.3035,-218.7938 488.8313,-208.2121 481.8198,-216.1551 488.3035,-218.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"496.4121\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"579.2432\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M489.2659,-257.8955C497.7059,-252.2887 506.9317,-246.0042 515.2432,-240 526.4263,-231.9213 538.41,-222.7141 548.9487,-214.4211\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"551.3826,-216.9582 557.0479,-208.0053 547.036,-211.4712 551.3826,-216.9582\"/>\n",
       "<text text-anchor=\"middle\" x=\"552.2949\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"543.2432\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M571.1225,-171.7289C568.6357,-166.0614 565.9142,-159.785 563.4775,-154 560.4286,-146.7615 557.2055,-138.8989 554.2496,-131.5982\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"557.4079,-130.0712 550.4259,-122.1023 550.9145,-132.6859 557.4079,-130.0712\"/>\n",
       "<text text-anchor=\"middle\" x=\"578.626\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"633.2432\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M590.6952,-171.7616C598.1409,-159.9036 607.9796,-144.2345 616.3204,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"619.4541,-132.5422 621.8077,-122.2121 613.5258,-128.8198 619.4541,-132.5422\"/>\n",
       "<text text-anchor=\"middle\" x=\"626.188\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"594.2432\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M624.9723,-85.7616C619.6465,-74.0176 612.6255,-58.5355 606.6392,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"609.8198,-43.8739 602.5021,-36.2121 603.4447,-46.7649 609.8198,-43.8739\"/>\n",
       "<text text-anchor=\"middle\" x=\"628.2881\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"673.2432\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M641.7261,-85.7616C647.1885,-74.0176 654.3895,-58.5355 660.5293,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.7286,-46.7554 664.7724,-36.2121 657.3815,-43.8032 663.7286,-46.7554\"/>\n",
       "<text text-anchor=\"middle\" x=\"664.7949\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"332.2432\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.2432,-171.7616C332.2432,-160.3597 332.2432,-145.4342 332.2432,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.7433,-132.2121 332.2432,-122.2121 328.7433,-132.2121 335.7433,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.7949\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x119c90400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('SBAR', [Tree('WHNP', [Tree('WP', ['Who'])]), Tree('S', [Tree('VP', [Tree('VBD', ['knew']), Tree('SBAR', [Tree('IN', ['that']), Tree('S', [Tree('NP', [Tree('EX', ['there'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('NN', ['something'])]), Tree('SBAR', [Tree('WHNP', [Tree('WDT', ['that'])]), Tree('S', [Tree('VP', [Tree('MD', ['could']), Tree('VP', [Tree('VB', ['have']), Tree('VP', [Tree('VBN', ['helped']), Tree('S', [Tree('VP', [Tree('VB', ['protect']), Tree('NP', [Tree('PRP', ['me'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NNP', ['HPV'])])])])]), Tree('SBAR', [Tree('WHADVP', [Tree('WRB', ['when'])]), Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('QP', [Tree('CD', ['11']), Tree('CC', ['or']), Tree('CD', ['12'])])]), Tree(',', [',']), Tree('ADVP', [Tree('NN', ['way']), Tree('IN', ['before'])])])])])])])])])])])])])])])])]), Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('MD', ['would']), Tree('ADVP', [Tree('RB', ['even'])]), Tree('VP', [Tree('VB', ['be']), Tree('VP', [Tree('VBN', ['exposed']), Tree('PP', [Tree('TO', ['to']), Tree('NP', [Tree('PRP', ['it'])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(hpvDF[\"sentences\"][2])) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('VP',\n",
       "   'knew that there is something that could have helped protect me from HPV when I was 11 or 12 , way before')],\n",
       " [('VP',\n",
       "   'is something that could have helped protect me from HPV when I was 11 or 12 , way before')],\n",
       " [('VP',\n",
       "   'could have helped protect me from HPV when I was 11 or 12 , way before')],\n",
       " [('VP', 'have helped protect me from HPV when I was 11 or 12 , way before')],\n",
       " [('VP', 'helped protect me from HPV when I was 11 or 12 , way before')],\n",
       " [('VP', 'protect me from HPV')]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'VP', 'protect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                          ROOT                                                                                    \n",
      "                                                                                           |                                                                                       \n",
      "                                                                                           S                                                                                      \n",
      "       ____________________________________________________________________________________|____________________________________________________________________________________   \n",
      "     SBAR                                                                                                                            |              |                           | \n",
      "  ____|____                                                                                                                          |              |                           |  \n",
      " |         S                                                                                                                         |              |                           | \n",
      " |         |                                                                                                                         |              |                           |  \n",
      " |         VP                                                                                                                        |              |                           | \n",
      " |     ____|_____                                                                                                                    |              |                           |  \n",
      " |    |         SBAR                                                                                                                 |              |                           | \n",
      " |    |     _____|____                                                                                                               |              |                           |  \n",
      " |    |    |          S                                                                                                              |              |                           | \n",
      " |    |    |      ____|______                                                                                                        |              |                           |  \n",
      " |    |    |     |           VP                                                                                                      |              |                           | \n",
      " |    |    |     |     ______|______                                                                                                 |              |                           |  \n",
      " |    |    |     |    |             NP                                                                                               |              |                           | \n",
      " |    |    |     |    |       ______|_____                                                                                           |              |                           |  \n",
      " |    |    |     |    |      |           SBAR                                                                                        |              |                           | \n",
      " |    |    |     |    |      |       _____|__________                                                                                |              |                           |  \n",
      " |    |    |     |    |      |      |                S                                                                               |              |                           | \n",
      " |    |    |     |    |      |      |                |                                                                               |              |                           |  \n",
      " |    |    |     |    |      |      |                VP                                                                              |              |                           | \n",
      " |    |    |     |    |      |      |      __________|_____________                                                                  |              |                           |  \n",
      " |    |    |     |    |      |      |     |                        VP                                                                |              |                           | \n",
      " |    |    |     |    |      |      |     |     ___________________|_______________________                                          |              |                           |  \n",
      " |    |    |     |    |      |      |     |    |                                           VP                                        |              |                           | \n",
      " |    |    |     |    |      |      |     |    |      _____________________________________|____                                     |              |                           |  \n",
      " |    |    |     |    |      |      |     |    |     |             |                           SBAR                                  |              VP                          | \n",
      " |    |    |     |    |      |      |     |    |     |             |                  __________|________                            |     _________|___________                |  \n",
      " |    |    |     |    |      |      |     |    |     |             S                 |                   S                           |    |    |                VP              | \n",
      " |    |    |     |    |      |      |     |    |     |             |                 |      _____________|___                        |    |    |     ___________|___            |  \n",
      " |    |    |     |    |      |      |     |    |     |             VP                |     |                 VP                      |    |    |    |               VP          | \n",
      " |    |    |     |    |      |      |     |    |     |        _____|________         |     |     ____________|___________            |    |    |    |      _________|___        |  \n",
      " |    |    |     |    |      |      |     |    |     |       |     |        PP       |     |    |        NP      |       |           |    |    |    |     |             PP      | \n",
      " |    |    |     |    |      |      |     |    |     |       |     |    ____|___     |     |    |        |       |       |           |    |    |    |     |          ___|___    |  \n",
      "WHNP  |    |     NP   |      NP    WHNP   |    |     |       |     NP  |        NP WHADVP  NP   |        QP      |      ADVP         NP   |   ADVP  |     |         |       NP  | \n",
      " |    |    |     |    |      |      |     |    |     |       |     |   |        |    |     |    |     ___|___    |    ___|_____      |    |    |    |     |         |       |   |  \n",
      " WP  VBD   IN    EX  VBZ     NN    WDT    MD   VB   VBN      VB   PRP  IN      NNP  WRB   PRP  VBD   CD  CC  CD  ,   NN        IN   PRP   MD   RB   VB   VBN        TO     PRP  . \n",
      " |    |    |     |    |      |      |     |    |     |       |     |   |        |    |     |    |    |   |   |   |   |         |     |    |    |    |     |         |       |   |  \n",
      "Who  knew that there  is something that could have helped protect  me from     HPV  when   I   was   11  or  12  ,  way      before  I  would even  be exposed      to      it  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hpv_text= []\n",
    "for s in hpvDF[\"sentences\"][2]:\n",
    "    hpv_text+= [\" \".join(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x119c02488>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [28]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'WP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'WP',\n",
      "                 'word': 'Who'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'ccomp': [5],\n",
      "                                      'nsubj': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 28,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'advcl',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'knew'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'mark',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'that'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'EX',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'expl',\n",
      "                 'tag': 'EX',\n",
      "                 'word': 'there'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBZ',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'expl': [4],\n",
      "                                      'mark': [3],\n",
      "                                      'nsubj': [6]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'ccomp',\n",
      "                 'tag': 'VBZ',\n",
      "                 'word': 'is'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'acl:relcl': [10]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'something'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'WDT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'WDT',\n",
      "                 'word': 'that'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'MD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'aux',\n",
      "                 'tag': 'MD',\n",
      "                 'word': 'could'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'VB',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'aux',\n",
      "                 'tag': 'VB',\n",
      "                 'word': 'have'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [18],\n",
      "                                       'aux': [8, 9],\n",
      "                                       'ccomp': [11],\n",
      "                                       'nsubj': [7]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 6,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'acl:relcl',\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'helped'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'VB',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'dobj': [12],\n",
      "                                       'nmod': [14]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'ccomp',\n",
      "                  'tag': 'VB',\n",
      "                  'word': 'protect'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'dobj',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'me'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 14,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'from'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'case': [13]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'HPV'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'WRB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advmod',\n",
      "                  'tag': 'WRB',\n",
      "                  'word': 'when'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubj',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'I'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'VBD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'cop',\n",
      "                  'tag': 'VBD',\n",
      "                  'word': 'was'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advmod': [15, 22],\n",
      "                                       'cc': [19],\n",
      "                                       'conj': [20],\n",
      "                                       'cop': [17],\n",
      "                                       'nsubj': [16]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advcl',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '11'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'CC',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'cc',\n",
      "                  'tag': 'CC',\n",
      "                  'word': 'or'},\n",
      "             20: {'address': 20,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'conj',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '12'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'case': [23]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advmod',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'way'},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 22,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'before'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 28,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubjpass',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'I'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'MD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 28,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'aux',\n",
      "                  'tag': 'MD',\n",
      "                  'word': 'would'},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 28,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advmod',\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'even'},\n",
      "             27: {'address': 27,\n",
      "                  'ctag': 'VB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 28,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'auxpass',\n",
      "                  'tag': 'VB',\n",
      "                  'word': 'be'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [2],\n",
      "                                       'advmod': [26],\n",
      "                                       'aux': [25],\n",
      "                                       'auxpass': [27],\n",
      "                                       'nmod': [30],\n",
      "                                       'nsubjpass': [24]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 0,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'root',\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'exposed'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'TO',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 30,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'TO',\n",
      "                  'word': 'to'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'case': [29]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 28,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'it'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(hpv_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[3])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"811pt\" height=\"732pt\"\n",
       " viewBox=\"0.00 0.00 810.93 732.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 728)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-728 806.9312,-728 806.9312,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"277.519\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"277.519\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (exposed)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;28 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M277.519,-687.7616C277.519,-676.3597 277.519,-661.4342 277.519,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.0191,-648.2121 277.519,-638.2121 274.0191,-648.2121 281.0191,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"288.7949\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"69.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (knew)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;2 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>28&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M233.0405,-611.7992C206.5076,-605.9851 172.5719,-596.9032 144.2017,-584 128.9387,-577.0582 113.3301,-567.134 100.3553,-557.9544\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.3896,-555.1062 92.2396,-552.0685 98.2799,-560.7728 102.3896,-555.1062\"/>\n",
       "<text text-anchor=\"middle\" x=\"159.6777\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"147.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>28&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M233.2624,-604.9061C220.0477,-599.3677 205.9338,-592.3655 194.0708,-584 184.6688,-577.3699 175.7083,-568.5857 168.2046,-560.2317\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.6688,-557.7296 161.4898,-552.4383 165.3657,-562.2988 170.6688,-557.7296\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.7432\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"231.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (would)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M267.7636,-601.7616C261.482,-590.0176 253.2008,-574.5355 246.14,-561.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"249.0632,-559.3792 241.2604,-552.2121 242.8907,-562.6808 249.0632,-559.3792\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.626\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"323.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (even)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M287.2745,-601.7616C293.5561,-590.0176 301.8373,-574.5355 308.8981,-561.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"312.1474,-562.6808 313.7777,-552.2121 305.9749,-559.3792 312.1474,-562.6808\"/>\n",
       "<text text-anchor=\"middle\" x=\"326.0708\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"404.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (be)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M321.2709,-601.882C331.87,-596.723 342.9077,-590.6698 352.519,-584 362.7054,-576.9311 372.79,-567.852 381.328,-559.3746\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"383.908,-561.7423 388.3967,-552.1453 378.903,-556.8484 383.908,-561.7423\"/>\n",
       "<text text-anchor=\"middle\" x=\"392.6812\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"477.519\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (it)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;30 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>28&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M321.929,-613.9598C350.6579,-608.7996 388.2113,-599.6438 418.519,-584 430.9458,-577.5857 443.0273,-568.1141 452.9873,-559.1347\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.5831,-561.4988 460.4863,-552.1068 450.7964,-556.3912 455.5831,-561.4988\"/>\n",
       "<text text-anchor=\"middle\" x=\"456.4639\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"31.519\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Who)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.7325,-515.7333C50.8604,-510.2754 46.9889,-504.1116 44.1812,-498 41.0305,-491.1421 38.5819,-483.3803 36.7147,-476.0532\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"40.0861,-475.0948 34.4495,-466.1238 33.2614,-476.6518 40.0861,-475.0948\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.688\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"108.519\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (is)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M77.7899,-515.7616C83.1157,-504.0176 90.1367,-488.5355 96.123,-475.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.3175,-476.7649 100.2601,-466.2121 92.9424,-473.8739 99.3175,-476.7649\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.1777\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"30.519\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (that)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M91.8397,-429.6955C86.6793,-424.0266 80.9782,-417.7578 75.7534,-412 68.5558,-404.0681 60.7247,-395.4187 53.6324,-387.5781\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.1404,-385.1332 46.8372,-380.0636 50.9483,-389.8282 56.1404,-385.1332\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.9019\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"108.519\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (there)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M108.519,-429.7616C108.519,-418.3597 108.519,-403.4342 108.519,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.0191,-390.2121 108.519,-380.2121 105.0191,-390.2121 112.0191,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.5708\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">expl</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"205.519\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (something)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M129.0902,-429.7616C143.108,-417.3335 161.8476,-400.719 177.2623,-387.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"179.8168,-389.4651 184.9775,-380.2121 175.1729,-384.2273 179.8168,-389.4651\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.688\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"205.519\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (helped)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;10 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M205.519,-343.7616C205.519,-332.3597 205.519,-317.4342 205.519,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0191,-304.2121 205.519,-294.2121 202.0191,-304.2121 209.0191,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"228.0571\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"42.519\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (that)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;7 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M165.4208,-259.1894C152.4215,-253.4337 138.0423,-246.7388 125.1812,-240 109.9476,-232.0181 93.6556,-222.4087 79.597,-213.7561\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"81.0553,-210.5415 70.7145,-208.2349 77.3599,-216.4866 81.0553,-210.5415\"/>\n",
       "<text text-anchor=\"middle\" x=\"140.688\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"122.519\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (could)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M187.9169,-257.7616C176.0323,-245.4475 160.1813,-229.0235 147.0613,-215.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"149.5587,-212.977 140.0958,-208.2121 144.5219,-217.8382 149.5587,-212.977\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.626\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"205.519\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (have)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M205.519,-257.7616C205.519,-246.3597 205.519,-231.4342 205.519,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0191,-218.2121 205.519,-208.2121 202.0191,-218.2121 209.0191,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.626\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"295.519\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (protect)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M224.6057,-257.7616C237.4926,-245.4475 254.6805,-229.0235 268.907,-215.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"271.6479,-217.6512 276.4599,-208.2121 266.8119,-212.5902 271.6479,-217.6512\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.1777\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"539.519\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (11)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;18 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M245.8072,-265.6264C310.7271,-248.9105 437.6225,-216.2368 501.2382,-199.8567\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"502.4414,-203.1612 511.2528,-197.2781 500.6959,-196.3823 502.4414,-203.1612\"/>\n",
       "<text text-anchor=\"middle\" x=\"407.6777\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"212.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (HPV)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;14 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M277.1605,-171.7847C271.5045,-166.1195 265.2801,-159.8303 259.6294,-154 252.0344,-146.1636 243.8503,-137.5415 236.4684,-129.6965\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8002,-127.0664 229.4055,-122.1683 233.6952,-131.8559 238.8002,-127.0664\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.4639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"295.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (me)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M295.519,-171.7616C295.519,-160.3597 295.519,-145.4342 295.519,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"299.0191,-132.2121 295.519,-122.2121 292.0191,-132.2121 299.0191,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"380.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (when)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;15 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>18&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M511.1518,-181.9587C491.4488,-175.7637 464.9682,-166.1986 443.4155,-154 430.9361,-146.9368 418.3373,-137.4839 407.7273,-128.6994\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"409.9226,-125.9719 400.0362,-122.1627 405.3894,-131.3058 409.9226,-125.9719\"/>\n",
       "<text text-anchor=\"middle\" x=\"466.0708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"462.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (I)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;16 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>18&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M521.8518,-171.8702C516.4362,-166.2084 510.5043,-159.8997 505.1812,-154 498.3324,-146.4095 491.0521,-138.0079 484.4811,-130.2925\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"486.8153,-127.6327 477.6833,-122.2608 481.4721,-132.155 486.8153,-127.6327\"/>\n",
       "<text text-anchor=\"middle\" x=\"519.688\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"539.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (was)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M539.519,-171.7616C539.519,-160.3597 539.519,-145.4342 539.519,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"543.0191,-132.2121 539.519,-122.2121 536.0191,-132.2121 543.0191,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"549.626\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"617.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (or)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M556.0608,-171.7616C567.126,-159.5615 581.8501,-143.3273 594.1121,-129.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"596.8754,-131.9707 601.0011,-122.2121 591.6904,-127.268 596.8754,-131.9707\"/>\n",
       "<text text-anchor=\"middle\" x=\"588.7329\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"695.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (way)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>18&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M567.7695,-174.426C591.7505,-161.2057 626.4995,-142.0492 653.704,-127.0519\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"655.5465,-130.0328 662.6142,-122.1399 652.167,-123.9026 655.5465,-130.0328\"/>\n",
       "<text text-anchor=\"middle\" x=\"649.0708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"774.519\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">20 (12)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>18&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M567.9273,-184.2971C595.8213,-178.2769 639.3323,-167.7542 675.519,-154 696.364,-146.077 718.61,-135.0897 736.7233,-125.4321\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"738.6601,-128.3637 745.7894,-120.5263 735.3287,-122.2072 738.6601,-128.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"717.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"212.519\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (from)</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M212.519,-85.7616C212.519,-74.3597 212.519,-59.4342 212.519,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"216.0191,-46.2121 212.519,-36.2121 209.0191,-46.2121 216.0191,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"224.564\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"695.519\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (before)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M695.519,-85.7616C695.519,-74.3597 695.519,-59.4342 695.519,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"699.0191,-46.2121 695.519,-36.2121 692.0191,-46.2121 699.0191,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"707.564\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"477.519\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (to)</text>\n",
       "</g>\n",
       "<!-- 30&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>30&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M477.519,-515.7616C477.519,-504.3597 477.519,-489.4342 477.519,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"481.0191,-476.2121 477.519,-466.2121 474.0191,-476.2121 481.0191,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"489.564\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x119c38828>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 13.06 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [14.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0111 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/86/kbyspkys411_n8krrz1xl9cc0000gn/T/tmpeov5kymd\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25    was African American from\n",
       "26            was American from\n",
       "27                          was\n",
       "28                          was\n",
       "Name: verb, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Trayvon Benjamin Martin']['verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25             Florida\n",
       "26             Florida\n",
       "27            American\n",
       "28    African American\n",
       "Name: object, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Trayvon Benjamin Martin']['object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> This operation is useful on the one hand but also limited on the other. I need to put in exact words to search for the corresponding objects and verbs. For example, if I search \"Trayvon Martin\" rather than \"Trayvon Benjamin Martin\", then I cannot find anything. This method is only useful for very specific tasks with a clearn goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction Continues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 17.87 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [19.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0128 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/86/kbyspkys411_n8krrz1xl9cc0000gn/T/tmp7dao_kq6\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>Quite often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>many homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>repeat offenders</td>\n",
       "      <td>is with</td>\n",
       "      <td>long ticket histories of types of issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>speak with</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>So anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak with machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>however was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780294</td>\n",
       "      <td>handling</td>\n",
       "      <td>types of</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy again for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our front entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put to</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>was</td>\n",
       "      <td>framed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>just going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                subject                            verb  \\\n",
       "0     1.000000                     we                         'll get   \n",
       "1     1.000000                     we             Quite often 'll get   \n",
       "2     1.000000                     we                   often 'll get   \n",
       "3     0.831036                     we                            coax   \n",
       "4     0.774359  straight analog cable                            coax   \n",
       "5     0.774359           analog cable                            coax   \n",
       "6     0.774359  straight analog cable                            coax   \n",
       "7     1.000000                     we    would supply analog cable to   \n",
       "8     0.831036                     we                            coax   \n",
       "9     0.774359           analog cable                            coax   \n",
       "10    0.831036                     we                            coax   \n",
       "11    0.774359  straight analog cable                            coax   \n",
       "12    0.774359  straight analog cable                            coax   \n",
       "13    0.831036                     we                            coax   \n",
       "14    0.774359           analog cable                            coax   \n",
       "15    1.000000                     we    would supply analog cable to   \n",
       "16    0.774359           analog cable                            coax   \n",
       "17    1.000000          our equipment                         receive   \n",
       "18    1.000000  our digital equipment                         receive   \n",
       "19    1.000000       repeat offenders                         is with   \n",
       "20    1.000000                      I                      anyway get   \n",
       "21    1.000000                      I                             get   \n",
       "22    1.000000                     he                      speak with   \n",
       "23    1.000000                      I                   So anyway get   \n",
       "24    1.000000                     he                             has   \n",
       "25    1.000000                     he                             has   \n",
       "26    1.000000                   call               however was going   \n",
       "27    1.000000                   call                       was going   \n",
       "28    0.780294               handling                        types of   \n",
       "29    1.000000                   call                       was going   \n",
       "..         ...                    ...                             ...   \n",
       "161   1.000000                     it                            made   \n",
       "162   1.000000                     it                            made   \n",
       "163   1.000000                     it                     really made   \n",
       "164   1.000000                    man                       happy for   \n",
       "165   1.000000                old man                 happy again for   \n",
       "166   1.000000                    man                 happy again for   \n",
       "167   1.000000                     it                     really made   \n",
       "168   1.000000                old man                 happy again for   \n",
       "169   1.000000                     it                     really made   \n",
       "170   1.000000                    man                       happy for   \n",
       "171   1.000000                     it                            made   \n",
       "172   1.000000                     it                            made   \n",
       "173   1.000000                     it                     really made   \n",
       "174   1.000000                     it                     really made   \n",
       "175   1.000000                old man                       happy for   \n",
       "176   1.000000                     it                     really made   \n",
       "177   1.000000                     it                            made   \n",
       "178   1.000000                     it                            made   \n",
       "179   1.000000                     it                     really made   \n",
       "180   1.000000                     it                            made   \n",
       "181   1.000000                 letter                          put on   \n",
       "182   1.000000                 letter                          put to   \n",
       "183   1.000000                 letter                          put on   \n",
       "184   1.000000                 letter                             was   \n",
       "185   1.000000                   they                   going through   \n",
       "186   1.000000                   they              just going through   \n",
       "187   1.000000                      I               still think about   \n",
       "188   1.000000                      I  still think occasionally about   \n",
       "189   1.000000                      I        think occasionally about   \n",
       "190   1.000000                      I                     think about   \n",
       "\n",
       "                                             object  \n",
       "0                                             calls  \n",
       "1                                             calls  \n",
       "2                                             calls  \n",
       "3                                      direct to TV  \n",
       "4                                  direct from wall  \n",
       "5                            direct from wall to TV  \n",
       "6                                      direct to TV  \n",
       "7                                             homes  \n",
       "8                                  direct from wall  \n",
       "9                                  direct from wall  \n",
       "10                           direct from wall to TV  \n",
       "11                           direct from wall to TV  \n",
       "12                                           direct  \n",
       "13                                           direct  \n",
       "14                                     direct to TV  \n",
       "15                                       many homes  \n",
       "16                                           direct  \n",
       "17                                         channels  \n",
       "18                                         channels  \n",
       "19         long ticket histories of types of issues  \n",
       "20                                             call  \n",
       "21                                             call  \n",
       "22                                          machine  \n",
       "23                                             call  \n",
       "24                                            speak  \n",
       "25                               speak with machine  \n",
       "26                                 little different  \n",
       "27                                 little different  \n",
       "28                                        customers  \n",
       "29                                        different  \n",
       "..                                              ...  \n",
       "161                                       man happy  \n",
       "162           man happy again for once in long time  \n",
       "163                  old man happy for once in time  \n",
       "164                                            once  \n",
       "165                                    once in time  \n",
       "166                                            once  \n",
       "167                                   old man happy  \n",
       "168                          once in very long time  \n",
       "169                                 man happy again  \n",
       "170                               once in long time  \n",
       "171            old man happy again for once in time  \n",
       "172                        man happy again for once  \n",
       "173  old man happy again for once in very long time  \n",
       "174       old man happy again for once in long time  \n",
       "175                                    once in time  \n",
       "176                      man happy for once in time  \n",
       "177            man happy for once in very long time  \n",
       "178        old man happy for once in very long time  \n",
       "179           man happy again for once in long time  \n",
       "180      man happy again for once in very long time  \n",
       "181                              our front entrance  \n",
       "182                                          retail  \n",
       "183                                    our entrance  \n",
       "184                                          framed  \n",
       "185                                             lot  \n",
       "186                                             lot  \n",
       "187                                       Mr. Smith  \n",
       "188                                       Mr. Smith  \n",
       "189                                       Mr. Smith  \n",
       "190                                       Mr. Smith  \n",
       "\n",
       "[191 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                        48\n",
       "it                       42\n",
       "he                       19\n",
       "He                       18\n",
       "we                       11\n",
       "old man                   8\n",
       "man                       8\n",
       "analog cable              4\n",
       "our booking calendar      4\n",
       "letter                    4\n",
       "straight analog cable     4\n",
       "call                      4\n",
       "my supervisor             3\n",
       "his TV set                2\n",
       "TV                        2\n",
       "you                       2\n",
       "they                      2\n",
       "repeat offenders          1\n",
       "me                        1\n",
       "our digital equipment     1\n",
       "handling                  1\n",
       "people                    1\n",
       "our equipment             1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "could come                        8\n",
       "brought                           5\n",
       "even brought                      5\n",
       "was                               4\n",
       "had                               4\n",
       "speak for                         3\n",
       "speak with                        1\n",
       "had cable within                  1\n",
       "do                                1\n",
       "felt                              1\n",
       "get to                            1\n",
       "have                              1\n",
       "So anyway get                     1\n",
       "anyway get                        1\n",
       "still think occasionally about    1\n",
       "'ve dealt with                    1\n",
       "think about                       1\n",
       "think occasionally about          1\n",
       "ask                               1\n",
       "took                              1\n",
       "get                               1\n",
       "instantly felt                    1\n",
       "still think about                 1\n",
       "complaint in                      1\n",
       "eventually had                    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Smith                                             4\n",
       "call                                                  3\n",
       "him                                                   3\n",
       "simplified remote                                     2\n",
       "remote                                                2\n",
       "simplified remote for his set top box                 2\n",
       "willing                                               2\n",
       "this                                                  2\n",
       "remote for his set top box                            2\n",
       "bad                                                   2\n",
       "get                                                   2\n",
       "speak for bit about account                           1\n",
       "useless                                               1\n",
       "her                                                   1\n",
       "speak with her for bit                                1\n",
       "bit about account for Mr. Smith                       1\n",
       "cable                                                 1\n",
       "book                                                  1\n",
       "speak with her                                        1\n",
       "speak for bit about account for Mr. Smith             1\n",
       "speak for bit                                         1\n",
       "speak with her for bit about account for Mr. Smith    1\n",
       "speak with her for bit about account                  1\n",
       "bit about account                                     1\n",
       "30 seconds                                            1\n",
       "experience                                            1\n",
       "residence                                             1\n",
       "how useless                                           1\n",
       "cable running                                         1\n",
       "bit                                                   1\n",
       "speak                                                 1\n",
       "cable running again                                   1\n",
       "it                                                    1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the corenlp server. When you run this server (with the command below), you can click on the browswer link provided to experiment with it. Note that when we run the server, executing the command below, it interrupts the current jupyter process and you will not be able to run code here again (processes will \"hang\" and never finish) until you interrup the process by clicking \"Kernel\" and then \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://localhost:16432 , please wait a few seconds\n",
      "\n",
      "Exiting (ノ≧▽≦)ノ\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> I sum up the text of the 9 commercials into one long text, which is then used as the input for the SAO analysis. \n",
    "\n",
    "<span style=\"color:green\"> \"Side effects\" as the subject correspond to \"pain at injection site\", \"itching\", \"pain\", \"swelling\", and \"redness\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text= \"\"\n",
    "for ad in hpvDF[\"ad\"]:\n",
    "    all_text+= ad.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 15.641 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [17.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0116 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/86/kbyspkys411_n8krrz1xl9cc0000gn/T/tmps53c735w\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>have</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>have</td>\n",
       "      <td>cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>swelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311903</td>\n",
       "      <td>redness</td>\n",
       "      <td>pain at</td>\n",
       "      <td>injection site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain at injection site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>itching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain at injection site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>swelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>itching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue with</td>\n",
       "      <td>cervical cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue with</td>\n",
       "      <td>cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue with</td>\n",
       "      <td>routine cervical cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>is</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue with</td>\n",
       "      <td>routine cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>vaccinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>chose</td>\n",
       "      <td>vaccinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>vaccinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>chose</td>\n",
       "      <td>vaccinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>swelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>side effects</td>\n",
       "      <td>include</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.311903</td>\n",
       "      <td>redness</td>\n",
       "      <td>pain at</td>\n",
       "      <td>injection site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>redness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>pain at injection site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>effects</td>\n",
       "      <td>include</td>\n",
       "      <td>itching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>two types</td>\n",
       "      <td>cause</td>\n",
       "      <td>seventy percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>two types</td>\n",
       "      <td>cause</td>\n",
       "      <td>seventy percent of cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>chose</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>get</td>\n",
       "      <td>my daughter vaccinated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>cdc</td>\n",
       "      <td>recommends</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue</td>\n",
       "      <td>cervical cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue</td>\n",
       "      <td>routine cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue</td>\n",
       "      <td>routine cervical cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>is</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>continue</td>\n",
       "      <td>cancer screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one women affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one less women affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one less women affected by cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one women affected by cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one less women affected by cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one less women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one less women affected by cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one women affected by cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>chose</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.014332</td>\n",
       "      <td>my daughter</td>\n",
       "      <td>vaccinated</td>\n",
       "      <td>i want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one less women affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one women affected by cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one women affected by cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one less women affected by cervical cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>her</td>\n",
       "      <td>be</td>\n",
       "      <td>one women affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.031911</td>\n",
       "      <td>i</td>\n",
       "      <td>want</td>\n",
       "      <td>one less women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>daughter</td>\n",
       "      <td>has</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty       subject           verb  \\\n",
       "0     1.000000          they           have   \n",
       "1     1.000000          they           have   \n",
       "2     1.000000       effects        include   \n",
       "3     1.000000  side effects        include   \n",
       "4     0.311903       redness        pain at   \n",
       "5     1.000000       effects        include   \n",
       "6     1.000000  side effects        include   \n",
       "7     1.000000       effects        include   \n",
       "8     1.000000       effects        include   \n",
       "9     1.000000  side effects        include   \n",
       "10    1.000000  side effects        include   \n",
       "11    1.000000       effects        include   \n",
       "12    1.000000  side effects        include   \n",
       "13    1.000000            it  continue with   \n",
       "14    1.000000            it  continue with   \n",
       "15    1.000000            it  continue with   \n",
       "16    1.000000            it             is   \n",
       "17    1.000000            it  continue with   \n",
       "18    1.000000             i           want   \n",
       "19    1.000000             i            get   \n",
       "20    1.000000             i          chose   \n",
       "21    1.000000             i            get   \n",
       "22    1.000000             i          chose   \n",
       "23    1.000000       effects        include   \n",
       "24    1.000000  side effects        include   \n",
       "25    1.000000  side effects        include   \n",
       "26    0.311903       redness        pain at   \n",
       "27    1.000000       effects        include   \n",
       "28    1.000000       effects        include   \n",
       "29    1.000000       effects        include   \n",
       "..         ...           ...            ...   \n",
       "176   1.000000     two types          cause   \n",
       "177   1.000000     two types          cause   \n",
       "178   1.000000             i          chose   \n",
       "179   1.000000             i            get   \n",
       "180   1.000000           cdc     recommends   \n",
       "181   1.000000            it       continue   \n",
       "182   1.000000            it       continue   \n",
       "183   1.000000            it       continue   \n",
       "184   1.000000            it             is   \n",
       "185   1.000000            it       continue   \n",
       "186   0.031911             i           want   \n",
       "187   1.000000           her             be   \n",
       "188   0.031911             i           want   \n",
       "189   0.031911             i           want   \n",
       "190   1.000000           her             be   \n",
       "191   1.000000           her             be   \n",
       "192   1.000000           her             be   \n",
       "193   0.031911             i           want   \n",
       "194   1.000000           her             be   \n",
       "195   1.000000             i          chose   \n",
       "196   0.014332   my daughter     vaccinated   \n",
       "197   1.000000           her             be   \n",
       "198   0.031911             i           want   \n",
       "199   1.000000             i           want   \n",
       "200   0.031911             i           want   \n",
       "201   0.031911             i           want   \n",
       "202   1.000000           her             be   \n",
       "203   1.000000           her             be   \n",
       "204   0.031911             i           want   \n",
       "205   1.000000      daughter            has   \n",
       "\n",
       "                                         object  \n",
       "0                                        cancer  \n",
       "1                               cervical cancer  \n",
       "2                                      swelling  \n",
       "3                                          pain  \n",
       "4                                injection site  \n",
       "5                                       redness  \n",
       "6                                       redness  \n",
       "7                        pain at injection site  \n",
       "8                                       itching  \n",
       "9                        pain at injection site  \n",
       "10                                     swelling  \n",
       "11                                         pain  \n",
       "12                                      itching  \n",
       "13                    cervical cancer screening  \n",
       "14                             cancer screening  \n",
       "15            routine cervical cancer screening  \n",
       "16                                    important  \n",
       "17                     routine cancer screening  \n",
       "18                                     one less  \n",
       "19                                   vaccinated  \n",
       "20                                   vaccinated  \n",
       "21                                   vaccinated  \n",
       "22                                   vaccinated  \n",
       "23                                     swelling  \n",
       "24                                         pain  \n",
       "25                                      redness  \n",
       "26                               injection site  \n",
       "27                                      redness  \n",
       "28                       pain at injection site  \n",
       "29                                      itching  \n",
       "..                                          ...  \n",
       "176                             seventy percent  \n",
       "177          seventy percent of cervical cancer  \n",
       "178                                         get  \n",
       "179                      my daughter vaccinated  \n",
       "180                                        that  \n",
       "181                   cervical cancer screening  \n",
       "182                    routine cancer screening  \n",
       "183           routine cervical cancer screening  \n",
       "184                                   important  \n",
       "185                            cancer screening  \n",
       "186                          one women affected  \n",
       "187                                   one women  \n",
       "188                     one less women affected  \n",
       "189           one less women affected by cancer  \n",
       "190                one women affected by cancer  \n",
       "191           one less women affected by cancer  \n",
       "192                              one less women  \n",
       "193  one less women affected by cervical cancer  \n",
       "194       one women affected by cervical cancer  \n",
       "195                                         get  \n",
       "196                                      i want  \n",
       "197                     one less women affected  \n",
       "198       one women affected by cervical cancer  \n",
       "199                                         her  \n",
       "200                one women affected by cancer  \n",
       "201                                   one women  \n",
       "202  one less women affected by cervical cancer  \n",
       "203                          one women affected  \n",
       "204                              one less women  \n",
       "205                                    daughter  \n",
       "\n",
       "[206 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i                      44\n",
       "it                     24\n",
       "winnie                 12\n",
       "gardasil               12\n",
       "side effects           10\n",
       "effects                10\n",
       "you                     9\n",
       "her                     8\n",
       "who knew                6\n",
       "my mom                  5\n",
       "they                    5\n",
       "women                   4\n",
       "cervical cancer         4\n",
       "your choices            4\n",
       "two types               3\n",
       "types                   3\n",
       "cancer                  3\n",
       "hpv                     3\n",
       "nine girl               2\n",
       "three hundred women     2\n",
       "hpv vaccination         2\n",
       "information             2\n",
       "more information        2\n",
       "vaccine                 2\n",
       "redness                 2\n",
       "girl                    2\n",
       "hpv vaccine             2\n",
       "best                    2\n",
       "girls                   2\n",
       "your family             2\n",
       "ninety women            2\n",
       "decision                1\n",
       "parent                  1\n",
       "cancers                 1\n",
       "cdc                     1\n",
       "my daughter             1\n",
       "other diseases          1\n",
       "child                   1\n",
       "thirty women            1\n",
       "diseases                1\n",
       "daughter                1\n",
       "informed decision       1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pain at injection site    2\n",
       "itching                   2\n",
       "pain                      2\n",
       "swelling                  2\n",
       "redness                   2\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'side effects']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
